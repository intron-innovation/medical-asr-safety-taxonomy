{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec751c73",
   "metadata": {},
   "source": [
    "## Load cleaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c41089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all datasets merged: 120\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_datasets_df = pd.read_csv('data/final_120_sampled_medical_datasets.csv')\n",
    "print('Loaded all datasets merged:', len(all_datasets_df))\n",
    "\n",
    "# # select first 2 for testing\n",
    "# all_datasets_df = all_datasets_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42198246",
   "metadata": {},
   "source": [
    "## Phi-4 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eae354a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelechi/miniconda3/envs/phi4_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "import soundfile\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "\n",
    "# import datasets\n",
    "from datasets import load_dataset\n",
    "import torchcodec\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa2f768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelechi/miniconda3/envs/phi4_env/lib/python3.9/site-packages/transformers/models/auto/image_processing_auto.py:646: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Phi4MMModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "/home/kelechi/.cache/huggingface/modules/transformers_modules/kumapo/Phi-4-multimodal-instruct/76ce9ff33ee5d4be6d84c00d415f0dca01df6871/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model and processor\n",
    "model_path = \"kumapo/Phi-4-multimodal-instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True,\n",
    "    _attn_implementation='eager',\n",
    ").cuda()\n",
    "generation_config = GenerationConfig.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a4db97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing Audio:   0%|          | 0/2 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/phi4_env/lib/python3.9/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "Transcribing Audio: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:32<00:00, 76.24s/it]\n"
     ]
    }
   ],
   "source": [
    "user_prompt = '<|user|>'\n",
    "assistant_prompt = '<|assistant|>'\n",
    "prompt_suffix = '<|end|>'\n",
    "speech_prompt = \"Based on the attached audio, generate a comprehensive text transcription of the spoken content.\"\n",
    "\n",
    "def transcribe_file_chunked(audio_path, chunk_seconds=30, max_new_tokens=500):\n",
    "    if not os.path.exists(audio_path):\n",
    "        return f'FILE_NOT_FOUND: {audio_path}'\n",
    "    try:\n",
    "        data, sr = soundfile.read(audio_path)  # (numpy array, sample_rate)\n",
    "        # ensure mono (if stereo, average channels)\n",
    "        if data.ndim > 1:\n",
    "            data = data.mean(axis=1)\n",
    "        total_samples = data.shape[0]\n",
    "        chunk_samples = int(chunk_seconds * sr)\n",
    "        if chunk_samples <= 0:\n",
    "            return \"ERROR: invalid chunk_seconds\"\n",
    "        segments = []\n",
    "        for start in range(0, total_samples, chunk_samples):\n",
    "            seg = data[start : start + chunk_samples]\n",
    "            if seg.size == 0:\n",
    "                continue\n",
    "            segments.append((seg, sr))\n",
    "        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        texts = []\n",
    "        for i, seg in enumerate(segments):\n",
    "            prompt = f'{user_prompt}<|audio_1|>{speech_prompt}{prompt_suffix}{assistant_prompt}'\n",
    "            inputs = processor(text=prompt, audios=[seg], return_tensors='pt').to(device)\n",
    "            generate_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                generation_config=generation_config,\n",
    "                use_cache=False,\n",
    "                min_length=1,\n",
    "                top_p=1.0,\n",
    "                repetition_penalty=1.0,\n",
    "                length_penalty=1.0,\n",
    "                temperature=1.0,\n",
    "                do_sample=False,\n",
    "                num_beams=1,\n",
    "            )\n",
    "            # slice off prompt tokens\n",
    "            generate_ids = generate_ids[:, inputs['input_ids'].shape[1] : ]\n",
    "            resp = processor.batch_decode(\n",
    "                generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )[0].strip()\n",
    "            texts.append(resp)\n",
    "            # cleanup to reduce peak memory\n",
    "            try:\n",
    "                del inputs, generate_ids\n",
    "            except Exception:\n",
    "                pass\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        # join segment transcriptions (you can change separator)\n",
    "        return \" \".join(t for t in texts if t)\n",
    "    except Exception as e:\n",
    "        return f'ERROR: {e}'\n",
    "\n",
    "audio_path = all_datasets_df['audio_file'].tolist()\n",
    "all_datasets_df['Phi-4-ASR'] = [transcribe_file_chunked(path, chunk_seconds=30, max_new_tokens=500) for path in tqdm(audio_path, desc='Transcribing Audio')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2eaee6",
   "metadata": {},
   "source": [
    "## Whisper ASR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c53a42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# use a device id for pipeline (int) and a torch device string for .to()\n",
    "torch_device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_id = 0 if torch.cuda.is_available() else -1\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "# move the model to the proper device\n",
    "model.to(torch_device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# create the pipeline; we keep model/tokenizer/feature_extractor explicit\n",
    "# note: pass device as int (0 for cuda, -1 for cpu) to the pipeline\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device_id,\n",
    "    generate_kwargs={\n",
    "        'max_new_tokens': 256,\n",
    "        'num_beams': 1,\n",
    "        'do_sample': False,\n",
    "        'repetition_penalty': 1.0,\n",
    "        'language': 'english'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "449511e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper Audio Transcribing:   0%|          | 0/2 [00:00<?, ?it/s]Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n",
      "Whisper Audio Transcribing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:19<00:19, 19.73s/it]Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n",
      "Whisper Audio Transcribing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:38<00:00, 19.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run Whisper ASR on all audio files in the dataframe for both doctor and patient\n",
    "def run_whisper_asr(audio_paths, desc):\n",
    "    responses = []\n",
    "    for audio_path in tqdm(audio_paths, desc=desc):\n",
    "        try:\n",
    "            result = pipe(\n",
    "                audio_path, \n",
    "                return_timestamps=True, \n",
    "                chunk_length_s=30,\n",
    "                generate_kwargs={'language': 'english'}\n",
    "            )\n",
    "            responses.append(result.get(\"text\", \"\"))\n",
    "        except Exception as e:\n",
    "            responses.append(f'ERROR: {e}')\n",
    "    return responses\n",
    "\n",
    "# primock_datasets['Whisper-ASR-Doctor'] = run_whisper_asr(primock_datasets['doctor_audio_path'].tolist(), 'Whisper Doctor Transcribing')\n",
    "# primock_datasets['Whisper-ASR-Patient'] = run_whisper_asr(primock_datasets['patient_audio_path'].tolist(), 'Whisper Patient Transcribing')\n",
    "all_datasets_df['Whisper-ASR'] = run_whisper_asr(all_datasets_df['audio_file'].tolist(), 'Whisper Audio Transcribing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv\n",
    "all_datasets_df.to_csv('results/phi_4_asr_results_all.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi4_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
