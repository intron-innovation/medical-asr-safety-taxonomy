{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Previous approach using raw data processing.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvxc_47eE1lx",
        "outputId": "30ab2a4b-256e-452d-d0a4-7b0c563c97cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTBhRhS4JlUy"
      },
      "outputs": [],
      "source": [
        "!pip install nemo_toolkit['asr']\n",
        "!pip install torchcodec\n",
        "!pip install datasets huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctoTxgq8Ew1A"
      },
      "outputs": [],
      "source": [
        "import os, shutil, requests\n",
        "import torch\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "import nemo.collections.asr as nemo_asr\n",
        "import glob\n",
        "from datasets import load_dataset, Audio\n",
        "from datasets import load_from_disk\n",
        "from huggingface_hub import hf_hub_download\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mciw0FP9PIv4"
      },
      "source": [
        "## IBM-Granite\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK_TRwruidbQ"
      },
      "outputs": [],
      "source": [
        "# 1. UNIFIED DATASET LOADER\n",
        "def load_dataset_samples(dataset, base_path):\n",
        "    samples = []\n",
        "\n",
        "    # A. MERGED LOADER FOR: US_MEDICAL + AFRISPEECH\n",
        "    if dataset in [\"us_medical_45\", \"afrispeech\"]:\n",
        "        if dataset == \"us_medical_45\":\n",
        "            audio_folder = f\"{base_path}/data/{dataset}/audio\"\n",
        "            transcript_folder = f\"{base_path}/data/{dataset}/transcripts\"\n",
        "        else:\n",
        "            audio_folder = f\"{base_path}/data/afrispeech\"\n",
        "            transcript_folder = audio_folder   # transcripts in same folder\n",
        "\n",
        "        file_list = sorted([\n",
        "            f for f in os.listdir(audio_folder)\n",
        "            if f.lower().endswith((\".wav\", \".mp3\", \".flac\", \".ogg\", \".m4a\"))\n",
        "        ])\n",
        "\n",
        "        for fname in file_list:\n",
        "            uid = os.path.splitext(fname)[0]\n",
        "            audio_path = os.path.join(audio_folder, fname)\n",
        "            txt_path = os.path.join(transcript_folder, uid + \".txt\")\n",
        "\n",
        "            # optional transcript\n",
        "            transcript = \"\"\n",
        "            if os.path.exists(txt_path):\n",
        "                transcript = open(txt_path, \"r\", encoding=\"utf-8\").read().strip()\n",
        "\n",
        "            # load audio\n",
        "            try:\n",
        "                wav, sr = torchaudio.load(audio_path, normalize=True)\n",
        "                if wav.shape[0] > 1:\n",
        "                    wav = wav.mean(dim=0).unsqueeze(0)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading audio {audio_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "            samples.append((uid, wav, sr, transcript, \"unknown\", None))\n",
        "\n",
        "        return samples\n",
        "\n",
        "    # B. PRIMOCK (doctor + patient audio inside CSV)\n",
        "    elif dataset == \"primock\":\n",
        "\n",
        "        csv_path = f\"{base_path}/data/Primock-57/primock57_details.csv\"\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        for row_idx, row in df.iterrows():\n",
        "            uid = row[\"utterance_id\"]\n",
        "\n",
        "            # DOCTOR\n",
        "            if pd.notna(row[\"doctor_audio_path\"]):\n",
        "                try:\n",
        "                    wav, sr = torchaudio.load(row[\"doctor_audio_path\"], normalize=True)\n",
        "                    if wav.shape[0] > 1:\n",
        "                        wav = wav.mean(dim=0).unsqueeze(0)\n",
        "                    transcript = row.get(\"doctor_utterances\", \"\")\n",
        "                    samples.append((uid, wav, sr, transcript, \"doctor\", row_idx))\n",
        "                except Exception as e:\n",
        "                    print(f\"Doctor audio error for {uid}: {e}\")\n",
        "\n",
        "            # PATIENT\n",
        "            if pd.notna(row[\"patient_audio_path\"]):\n",
        "                try:\n",
        "                    wav, sr = torchaudio.load(row[\"patient_audio_path\"], normalize=True)\n",
        "                    if wav.shape[0] > 1:\n",
        "                        wav = wav.mean(dim=0).unsqueeze(0)\n",
        "                    transcript = row.get(\"patient_utterances\", \"\")\n",
        "                    samples.append((uid, wav, sr, transcript, \"patient\", row_idx))\n",
        "                except Exception as e:\n",
        "                    print(f\"Patient audio error for {uid}: {e}\")\n",
        "\n",
        "        return samples\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xk7HmiFkad_W"
      },
      "outputs": [],
      "source": [
        "# 2. IBM Granite CONFIG\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dataset = \"primock\"   # <-- CHANGE HERE TO REQUIRED DATASET: us_medical_45 or afrispeech or primock\n",
        "base_path = \"/content/drive/MyDrive/Colab Notebooks/BioRAMP/ASR\"\n",
        "output_csv = f\"{base_path}/data/asr_results/{dataset}_granite_asr.csv\"\n",
        "\n",
        "# 3. LOAD MODEL\n",
        "print(\"Loading IBM Granite 8B...\")\n",
        "model_name = \"ibm-granite/granite-speech-3.3-8b\"\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "tokenizer = processor.tokenizer\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
        "\n",
        "system_prompt = (\n",
        "    \"Knowledge Cutoff Date: April 2024.\\n\"\n",
        "    \"Today's Date: April 9, 2025.\\n\"\n",
        "    \"You are Granite, developed by IBM. Transcribe speech verbatim.\"\n",
        ")\n",
        "user_prompt_base = \"<|audio|> Please transcribe the speech into written format.\"\n",
        "\n",
        "# 4. LOAD SAMPLES\n",
        "samples = load_dataset_samples(dataset, base_path)\n",
        "print(f\"Loaded {len(samples)} audio samples.\")\n",
        "\n",
        "# 5. SETUP PRIMOCK CSV IF NEEDED\n",
        "primock_df = None\n",
        "if dataset == \"primock\":\n",
        "    primock_path = f\"{base_path}/data/Primock-57/primock57_details.csv\"\n",
        "    primock_df = pd.read_csv(primock_path)\n",
        "    primock_df_results = primock_df.copy()\n",
        "    primock_df_results[\"IBM-Granite-doctor\"] = \"\"\n",
        "    primock_df_results[\"IBM-Granite-patient\"] = \"\"\n",
        "\n",
        "# 6. ASR LOOP (UNIFIED)\n",
        "records = []\n",
        "\n",
        "for uid, wav, sr, transcript_text, speaker, row_idx in samples:\n",
        "    print(\"Processing:\", uid, \"| Speaker:\", speaker)\n",
        "\n",
        "    # resample if needed\n",
        "    if sr != 16000:\n",
        "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
        "        sr = 16000\n",
        "\n",
        "    duration_sec = wav.shape[1] / sr\n",
        "\n",
        "    # Define the desired chunk size\n",
        "    chunk_size_seconds = 30\n",
        "    chunk_size_samples = chunk_size_seconds * 16000\n",
        "\n",
        "    # Split the waveform into chunks of equal size\n",
        "    chunks = torch.split(wav, chunk_size_samples, dim=1)\n",
        "\n",
        "    # RUN ASR\n",
        "    try:\n",
        "        chat = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt_base}\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        generated_texts = []\n",
        "\n",
        "        for chunk in tqdm(chunks, desc=\"Generating transcript...\"):\n",
        "            model_inputs = processor(\n",
        "                text,\n",
        "                chunk,\n",
        "                device=device, # Computation device; returned tensors are put on CPU\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(device)\n",
        "\n",
        "            # Generate\n",
        "            model_outputs = model.generate(\n",
        "                **model_inputs,\n",
        "                max_new_tokens=1000,\n",
        "                num_beams=1,\n",
        "                do_sample=False,\n",
        "                min_length=1,\n",
        "                top_p=1.0,\n",
        "                repetition_penalty=1.0,\n",
        "                length_penalty=1.0,\n",
        "                temperature=1.0,\n",
        "                bos_token_id=tokenizer.bos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                pad_token_id=tokenizer.pad_token_id,)\n",
        "\n",
        "            num_input_tokens = model_inputs[\"input_ids\"].shape[-1]\n",
        "            new_tokens = torch.unsqueeze(model_outputs[0, num_input_tokens:], dim=0)\n",
        "\n",
        "            output_text = tokenizer.batch_decode(new_tokens, add_special_tokens=False, skip_special_tokens=True)[0]\n",
        "            generated_texts.append(output_text)\n",
        "        granite_text = \" \".join(generated_texts)\n",
        "    except Exception as e:\n",
        "        granite_text = f\"ERROR: {e}\"\n",
        "        print(\"ASR Error:\", e)\n",
        "\n",
        "    # CASE 1: PRIMOCK — write into same CSV, no new file\n",
        "    if dataset == \"primock\":\n",
        "        if speaker == \"doctor\":\n",
        "            primock_df_results.at[row_idx, \"IBM-Granite-doctor\"] = granite_text\n",
        "        elif speaker == \"patient\":\n",
        "            primock_df_results.at[row_idx, \"IBM-Granite-patient\"] = granite_text\n",
        "\n",
        "    # CASE 2: OTHER DATASETS — save results to new CSV\n",
        "    else:\n",
        "        records.append({\n",
        "            \"utterance_id\": uid,\n",
        "            \"duration_sec\": round(duration_sec, 2),\n",
        "            \"human_transcript\": transcript_text,\n",
        "            \"source\": dataset,\n",
        "            \"IBM-Granite\": granite_text\n",
        "        })\n",
        "\n",
        "# 7. SAVE RESULTS\n",
        "# PRIMOCK → update same CSV\n",
        "if dataset == \"primock\":\n",
        "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
        "    primock_df_results.to_csv(output_csv, index=False)\n",
        "    print(\"\\n Primock CSV updated:\", output_csv)\n",
        "\n",
        "# OTHER DATASETS → save new CSV\n",
        "else:\n",
        "    df = pd.DataFrame(records)\n",
        "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"\\n {dataset} Saved ASR results:\", output_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UoMRwBxPDx_"
      },
      "source": [
        "## Nvidia-Parakeet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0bt3ZIfOap7"
      },
      "outputs": [],
      "source": [
        "# 1. CREATE JOB LIST FOR ALL DATASETS so we can run it in batches since it keeps timing out\n",
        "def get_job_list(dataset, base_path):\n",
        "    # CASE A — US_Medical and Afrispeech Dataset\n",
        "    if dataset in [\"us_medical_45\", \"afrispeech\"]:\n",
        "        if dataset == \"us_medical_45\":\n",
        "            audio_folder = f\"{base_path}/data/{dataset}/audio\"\n",
        "            transcript_folder = f\"{base_path}/data/{dataset}/transcripts\"\n",
        "        else:\n",
        "            audio_folder = f\"{base_path}/data/afrispeech\"\n",
        "            transcript_folder = audio_folder #the transcripts are in the same folder. modify this to your folder if needed\n",
        "\n",
        "        files = sorted([f for f in os.listdir(audio_folder) if f.lower().endswith((\".wav\", \".mp3\", \".flac\", \".ogg\", \".m4a\"))])\n",
        "\n",
        "        jobs = []\n",
        "        for fname in files:\n",
        "            uid = os.path.splitext(fname)[0]\n",
        "            audio_path = os.path.join(audio_folder, fname)\n",
        "\n",
        "            transcript = \"\"\n",
        "            for ext in [\".txt\", \".tsv\", \".csv\"]:\n",
        "                tpath = os.path.join(transcript_folder, uid + ext)\n",
        "                if os.path.exists(tpath):\n",
        "                    transcript = open(tpath).read().strip()\n",
        "                    break\n",
        "\n",
        "            jobs.append({\n",
        "                \"audio_path\": audio_path,\n",
        "                \"utterance_id\": uid,\n",
        "                \"transcript\": transcript,\n",
        "                \"speaker\": \"unknown\",\n",
        "                \"row_idx\": None\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(jobs)\n",
        "\n",
        "    # CASE B — PRIMOCK (doctor + patient per row)\n",
        "    elif dataset == \"primock\":\n",
        "        primock_path = f\"{base_path}/data/Primock-57/primock57_details.csv\"\n",
        "        df = pd.read_csv(primock_path)\n",
        "\n",
        "        jobs = []\n",
        "        for idx, row in df.iterrows():\n",
        "            # doctor\n",
        "            if pd.notna(row[\"doctor_audio_path\"]):\n",
        "                jobs.append({\n",
        "                    \"audio_path\": row[\"doctor_audio_path\"],\n",
        "                    \"utterance_id\": row[\"utterance_id\"],\n",
        "                    \"transcript\": row.get(\"doctor_utterances\", \"\"),\n",
        "                    \"speaker\": \"doctor\",\n",
        "                    \"row_idx\": idx\n",
        "                })\n",
        "\n",
        "            # patient\n",
        "            if pd.notna(row[\"patient_audio_path\"]):\n",
        "                jobs.append({\n",
        "                    \"audio_path\": row[\"patient_audio_path\"],\n",
        "                    \"utterance_id\": row[\"utterance_id\"],\n",
        "                    \"transcript\": row.get(\"patient_utterances\", \"\"),\n",
        "                    \"speaker\": \"patient\",\n",
        "                    \"row_idx\": idx\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(jobs)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported dataset: {dataset}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbK0OdeitiCC"
      },
      "outputs": [],
      "source": [
        "# 2. NVIDIA PARAKEET BATCH PIPELINE\n",
        "\n",
        "dataset = \"us_medical_45\"   # CHANGE THIS TO THE REQUIRED DATASET: us_medical_45 or afrispeech or primock\n",
        "base_path = \"/content/drive/MyDrive/Colab Notebooks/BioRAMP/ASR\"\n",
        "\n",
        "#set batch size to run in batches due to timeout observed when running all at once\n",
        "START = 0 #change to desired start index for batch\n",
        "BATCH_SIZE = 100 #change to the length of the file if you want to run all at once\n",
        "\n",
        "model_name = \"nvidia/parakeet-tdt-0.6b-v2\"\n",
        "nvidia_model = nemo_asr.models.ASRModel.from_pretrained(model_name)\n",
        "print(\"Loaded NVIDIA Parakeet\")\n",
        "\n",
        "# GET JOB LIST (BATCHABLE)\n",
        "job_df = get_job_list(dataset, base_path)\n",
        "batch = job_df.iloc[START: START + BATCH_SIZE]\n",
        "\n",
        "print(f\"\\nProcessing batch {START} → {START + len(batch)}\")\n",
        "print(f\"Total jobs available: {len(job_df)}\\n\")\n",
        "\n",
        "if dataset == \"primock\":\n",
        "    primock_path = f\"{base_path}/data/Primock-57/primock57_details.csv\"\n",
        "    primock_df = pd.read_csv(primock_path)\n",
        "\n",
        "    batch_idxs = batch[\"row_idx\"].unique()\n",
        "    primock_df_results = primock_df.loc[batch_idxs].copy().reset_index(drop=True)\n",
        "    idx_map = {orig: i for i, orig in enumerate(batch_idxs)}\n",
        "\n",
        "    if \"Nvidia-Parakeet-doctor\" not in primock_df_results.columns:\n",
        "        primock_df_results[\"Nvidia-Parakeet-doctor\"] = \"\"\n",
        "    if \"Nvidia-Parakeet-patient\" not in primock_df_results.columns:\n",
        "        primock_df_results[\"Nvidia-Parakeet-patient\"] = \"\"\n",
        "\n",
        "records = []\n",
        "\n",
        "# 3. PROCESS BATCH\n",
        "for _, job in batch.iterrows():\n",
        "    audio_path = job[\"audio_path\"]\n",
        "    uid        = job[\"utterance_id\"]\n",
        "    transcript = job[\"transcript\"]\n",
        "    speaker    = job[\"speaker\"]\n",
        "    row_idx    = job[\"row_idx\"]\n",
        "\n",
        "    print(\"--\", audio_path)\n",
        "\n",
        "    # RUN ASR\n",
        "    try:\n",
        "        asr_text = nvidia_model.transcribe([audio_path])[0].text\n",
        "    except Exception as e:\n",
        "        asr_text = f\"ERROR: {e}\"\n",
        "\n",
        "    # SAVE TO PRIMOCK RESULT COPY\n",
        "    if dataset == \"primock\":\n",
        "        local_idx = idx_map[row_idx]\n",
        "        if speaker == \"doctor\":\n",
        "            primock_df_results.at[local_idx, \"Nvidia-Parakeet-doctor\"] = asr_text\n",
        "        else:\n",
        "            primock_df_results.at[local_idx, \"Nvidia-Parakeet-patient\"] = asr_text\n",
        "\n",
        "    # Other datasets\n",
        "    else:\n",
        "        records.append({\n",
        "            \"utterance_id\": uid,\n",
        "            \"audio_file\": audio_path,\n",
        "            \"human_transcript\": transcript,\n",
        "            \"source\": dataset,\n",
        "            \"Nvidia-Parakeet\": asr_text\n",
        "        })\n",
        "\n",
        "# 4. SAVE OUTPUT\n",
        "if dataset == \"primock\":\n",
        "    outpath = f\"{base_path}/data/asr_results/batches/primock_nvidia_batch_{START}.csv\"\n",
        "    primock_df_results.to_csv(outpath, index=False)\n",
        "    print(\"\\n Saved Primock PARAKEET batch results:\", outpath)\n",
        "\n",
        "else:\n",
        "    outpath = f\"{base_path}/data/asr_results/batches/{dataset}_nvidia_batch_{START}.csv\"\n",
        "    pd.DataFrame(records).to_csv(outpath, index=False)\n",
        "    print(f\"\\n Saved {dataset} PARAKEET batch results::\", outpath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2pXUDFRzbOZ"
      },
      "outputs": [],
      "source": [
        "#combine the output from the batches into a single csv\n",
        "output_csv = f\"/content/drive/MyDrive/Colab Notebooks/BioRAMP/ASR/data/asr_results/{dataset}_nvidia_asr.csv\"\n",
        "files = glob.glob(f\"{base_path}/data/asr_results/batches/{dataset}_nvidia_batch_*.csv\")\n",
        "df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
        "\n",
        "df.to_csv(output_csv, index=False)\n",
        "print(f\"Merged {dataset}_nvidia_asr.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnxGKGZvNdfP"
      },
      "source": [
        "## Combine both models (Nvidia Parakeet and IBM Granite) in one csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc-C-91g0D3l"
      },
      "outputs": [],
      "source": [
        "dataset = \"us_medical_45\" # CHANGE THIS TO THE REQUIRED DATASET: us_medical_45 or afrispeech or primock\n",
        "nvidia_df = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/BioRAMP/ASR/data/asr_results/{dataset}_nvidia_asr.csv\")\n",
        "granite_df = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/BioRAMP/ASR/data/asr_results/{dataset}_granite_asr.csv\")\n",
        "\n",
        "if dataset == \"primock\":\n",
        "  print(\"hello\")\n",
        "  merged_df = nvidia_df.merge(\n",
        "      granite_df[['utterance_id', 'IBM-Granite-doctor', 'IBM-Granite-patient']],\n",
        "      on=\"utterance_id\",\n",
        "      how=\"inner\" \n",
        "  )\n",
        "\n",
        "else:\n",
        "  merged_df = nvidia_df.merge(\n",
        "      granite_df[['utterance_id', 'IBM-Granite', 'duration_sec']],\n",
        "      on=\"utterance_id\",\n",
        "      how=\"inner\"\n",
        "  )\n",
        "\n",
        "merged_df.to_csv(f\"/content/drive/MyDrive/Colab Notebooks/BioRAMP/ASR/data/asr_results/{dataset}_asr.csv\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "OR_SFKbfkK8i",
        "_iJmO6ZRLx5R",
        "0_NbfKx8Yixh"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
