{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f24816b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from jiwer import wer\n",
    "from jiwer import compute_measures\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ccebe",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "746792a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kelechi_phi_whisper = pd.read_excel('results/bio_ramp_asr_results.xlsx',engine='openpyxl')\n",
    "ashisa_results_1 = pd.read_csv('results/afrispeech_asr - afrispeech_asr.csv')\n",
    "aisha_results_2 = pd.read_csv('results/us_medical_20_asr - us_medical_20_asr.csv')\n",
    "primock_phi_whisper = pd.read_csv('/home/kelechi/bio_ramp_asr/results/phi_4_asr_results_primock.csv')\n",
    "primock_granite_parakeet = pd.read_csv('/home/kelechi/bio_ramp_asr/results/primock_asr - primock_asr.csv')\n",
    "\n",
    "# concat the two results\n",
    "aisha_granite_parakeet = pd.concat([ashisa_results_1, aisha_results_2], ignore_index=True, sort=False)\n",
    "\n",
    "evaluate_data = pd.merge(kelechi_phi_whisper, aisha_granite_parakeet[['utterance_id', 'Nvidia-Parakeet', 'IBM-Granite']], on='utterance_id', how='inner')\n",
    "evaluate_data['Phi-4-ASR'] = evaluate_data['Phi-4-ASR'].apply(lambda x: pd.NA if isinstance(x, str) and \"ERROR: CUDA out of memory\" in x else x)\n",
    "\n",
    "# Merge Primock results\n",
    "primock_merged = pd.merge(primock_phi_whisper, primock_granite_parakeet[['utterance_id', 'Nvidia-Parakeet-doctor', 'Nvidia-Parakeet-patient', 'IBM-Granite-doctor', 'IBM-Granite-patient']], on='utterance_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b3d14",
   "metadata": {},
   "source": [
    "## Text Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d34c1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_timestamps(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove timestamps matching patterns like:\n",
    "      - 03:18:98\n",
    "      - 00:00:001\n",
    "    (pattern: 1-2 digits ':' 1-2 digits ':' 1-3 digits)\n",
    "\n",
    "    Returns cleaned string with extra spaces collapsed.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # remove timestamp tokens\n",
    "    cleaned = re.sub(r'\\b\\d{1,2}:\\d{1,2}:\\d{1,3}\\b', '', text)\n",
    "    # remove new line characters (convert to spaces), then collapse multiple spaces and trim\n",
    "    cleaned = cleaned.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    \n",
    "    # remove speaker tags like: [Speaker 1]:  Speaker 1:  speaker1:  D:  P:\n",
    "    cleaned = re.sub(r'\\[?[Ss]peaker\\s*\\d+\\]?:', '', cleaned)\n",
    "    cleaned = re.sub(r'\\b[Dd]:', '', cleaned)\n",
    "    cleaned = re.sub(r'\\b[Pp]:', '', cleaned)\n",
    "    \n",
    "    # remove all characters that is not A-Z, a-z, 0-9 or space\n",
    "    cleaned = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", cleaned)\n",
    "    \n",
    "    # lowercase the text\n",
    "    cleaned = cleaned.lower()\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91277d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply remove_timestamps to the human transcript column\n",
    "evaluate_data['norm_human_transcript'] = evaluate_data['human-transcript'].apply(remove_timestamps)\n",
    "evaluate_data['norm_whisper_asr'] = evaluate_data['Whisper-ASR'].apply(remove_timestamps)\n",
    "evaluate_data['norm_phi4_asr'] = evaluate_data['Phi-4-ASR'].apply(remove_timestamps)\n",
    "evaluate_data['norm_parakeet'] = evaluate_data['Nvidia-Parakeet'].apply(remove_timestamps)\n",
    "evaluate_data['norm_granite'] = evaluate_data['IBM-Granite'].apply(remove_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14b7a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "primock_merged['norm_human_doctor'] = primock_merged['doctor_utterances'].apply(remove_timestamps)\n",
    "primock_merged['norm_human_patient'] = primock_merged['patient_utterances'].apply(remove_timestamps)\n",
    "primock_merged['norm_phi4_doctor'] = primock_merged['Phi-4-ASR-Doctor'].apply(remove_timestamps)\n",
    "primock_merged['norm_phi4_patient'] = primock_merged['Phi-4-ASR-Patient'].apply(remove_timestamps)\n",
    "primock_merged['norm_whisper_doctor'] = primock_merged['Whisper-ASR-Doctor'].apply(remove_timestamps)\n",
    "primock_merged['norm_whisper_patient'] = primock_merged['Whisper-ASR-Patient'].apply(remove_timestamps)\n",
    "primock_merged['norm_granite_doctor'] = primock_merged['IBM-Granite-doctor'].apply(remove_timestamps)\n",
    "primock_merged['norm_granite_patient'] = primock_merged['IBM-Granite-patient'].apply(remove_timestamps)\n",
    "primock_merged['norm_parakeet_doctor'] = primock_merged['Nvidia-Parakeet-doctor'].apply(remove_timestamps)\n",
    "primock_merged['norm_parakeet_patient'] = primock_merged['Nvidia-Parakeet-patient'].apply(remove_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040741f",
   "metadata": {},
   "source": [
    "## Calculate WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf63f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_columns = ['norm_whisper_asr', 'norm_phi4_asr', 'norm_parakeet', 'norm_granite']\n",
    "# Calculate WER for each ASR column\n",
    "for norm_col in norm_columns:\n",
    "    evaluate_data[f'{norm_col}_wer_compute'] = evaluate_data.apply(\n",
    "        lambda row: compute_measures(\n",
    "            row['norm_human_transcript'] if pd.notna(row['norm_human_transcript']) else \"\",\n",
    "            row[norm_col] if pd.notna(row[norm_col]) else \"\"\n",
    "        ), axis=1\n",
    "    )\n",
    "    evaluate_data[f'{norm_col}_wer'] = evaluate_data[f'{norm_col}_wer_compute'].apply(lambda measures: measures['wer'])\n",
    "    evaluate_data[f'{norm_col}_ins'] = evaluate_data[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['insertions']\n",
    "    )\n",
    "    evaluate_data[f'{norm_col}_del'] = evaluate_data[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['deletions']\n",
    "    )\n",
    "    evaluate_data[f'{norm_col}_sub'] = evaluate_data[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['substitutions']\n",
    "    )\n",
    "    evaluate_data[f'{norm_col}_ops'] = evaluate_data[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['ops']\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "094ae141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply wer calculation to primock as well\n",
    "primock_norm_columns = ['norm_whisper_doctor', 'norm_whisper_patient', 'norm_phi4_doctor', 'norm_phi4_patient',\n",
    "                        'norm_granite_doctor', 'norm_granite_patient', 'norm_parakeet_doctor', 'norm_parakeet_patient']\n",
    "for norm_col in primock_norm_columns:\n",
    "    primock_merged[f'{norm_col}_wer_compute'] = primock_merged.apply(\n",
    "        lambda row: compute_measures(\n",
    "            row['norm_human_doctor'] if 'doctor' in norm_col else row['norm_human_patient'],\n",
    "            row[norm_col] if pd.notna(row[norm_col]) else \"\"\n",
    "        ), axis=1\n",
    "    )\n",
    "    primock_merged[f'{norm_col}_wer'] = primock_merged[f'{norm_col}_wer_compute'].apply(lambda measures: measures['wer'])\n",
    "    primock_merged[f'{norm_col}_ins'] = primock_merged[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['insertions']\n",
    "    )\n",
    "    primock_merged[f'{norm_col}_del'] = primock_merged[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['deletions']\n",
    "    )\n",
    "    primock_merged[f'{norm_col}_sub'] = primock_merged[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['substitutions']\n",
    "    )\n",
    "    primock_merged[f'{norm_col}_ops'] = primock_merged[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['ops']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e4ae2",
   "metadata": {},
   "source": [
    "## Add Alignment for Sub, Del and Ins Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c7340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_alignment_ops(alignment_ops):\n",
    "    # If already a list/dict, return as-is (may be nested list)\n",
    "    if isinstance(alignment_ops, (list, dict)):\n",
    "        return alignment_ops\n",
    "    if alignment_ops is None or (isinstance(alignment_ops, float) and pd.isna(alignment_ops)):\n",
    "        return []\n",
    "    s = str(alignment_ops)\n",
    "    s = s.replace(\"AlignmentChunk(\", \"{\").replace(\")\", \"}\").replace(\"type=\", \"'type':\")\\\n",
    "         .replace(\"ref_start_idx=\", \"'ref_start_idx':\").replace(\"ref_end_idx=\", \"'ref_end_idx':\")\\\n",
    "         .replace(\"hyp_start_idx=\", \"'hyp_start_idx':\").replace(\"hyp_end_idx=\", \"'hyp_end_idx':\")\n",
    "    if s.startswith(\"[[\") and s.endswith(\"]]\"):\n",
    "        s = s[1:-1]\n",
    "    return s\n",
    "\n",
    "def _get_field(op, field, default=None):\n",
    "    # support dict-like\n",
    "    if isinstance(op, dict):\n",
    "        return op.get(field, default)\n",
    "    # support object with attribute (AlignmentChunk)\n",
    "    if hasattr(op, field):\n",
    "        return getattr(op, field, default)\n",
    "    # support tuple/list with numeric positions (fallback not used here)\n",
    "    return default\n",
    "\n",
    "def extract_words_from_alignment(ref_text, hyp_text, alignment_ops):\n",
    "    deletions, insertions, substitutions, equals = [], [], [], []\n",
    "    ref_words = ref_text.split() if isinstance(ref_text, str) else []\n",
    "    hyp_words = hyp_text.split() if isinstance(hyp_text, str) else []\n",
    "\n",
    "    # if hypothesis empty -> all deleted\n",
    "    if not hyp_words:\n",
    "        return ref_words, [], [], []\n",
    "\n",
    "    processed = preprocess_alignment_ops(alignment_ops)\n",
    "\n",
    "    # produce a flat list of op entries regardless of input shape\n",
    "    if isinstance(processed, list):\n",
    "        # flatten one level (handles [[...]] case)\n",
    "        flat = []\n",
    "        for item in processed:\n",
    "            if isinstance(item, list):\n",
    "                flat.extend(item)\n",
    "            else:\n",
    "                flat.append(item)\n",
    "        alignment_ops_list = flat\n",
    "    elif isinstance(processed, dict):\n",
    "        alignment_ops_list = [processed]\n",
    "    else:\n",
    "        if not processed:\n",
    "            return deletions, insertions, equals, substitutions\n",
    "        try:\n",
    "            alignment_ops_list = ast.literal_eval(processed)\n",
    "            # if parsed to nested list, flatten one level\n",
    "            if isinstance(alignment_ops_list, list) and any(isinstance(x, list) for x in alignment_ops_list):\n",
    "                flat = []\n",
    "                for item in alignment_ops_list:\n",
    "                    if isinstance(item, list):\n",
    "                        flat.extend(item)\n",
    "                    else:\n",
    "                        flat.append(item)\n",
    "                alignment_ops_list = flat\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse alignment_ops for row: {e}\")\n",
    "\n",
    "    for op in alignment_ops_list:\n",
    "        typ = _get_field(op, \"type\")\n",
    "        if typ == \"delete\":\n",
    "            s = _get_field(op, \"ref_start_idx\", 0)\n",
    "            e = _get_field(op, \"ref_end_idx\", 0)\n",
    "            deletions.extend(ref_words[s:e])\n",
    "        elif typ == \"equal\":\n",
    "            s = _get_field(op, \"hyp_start_idx\", 0)\n",
    "            e = _get_field(op, \"hyp_end_idx\", 0)\n",
    "            equals.extend(hyp_words[s:e])\n",
    "        elif typ == \"insert\":\n",
    "            s = _get_field(op, \"hyp_start_idx\", 0)\n",
    "            e = _get_field(op, \"hyp_end_idx\", 0)\n",
    "            insertions.extend(hyp_words[s:e])\n",
    "        elif typ == \"substitute\":\n",
    "            s = _get_field(op, \"ref_start_idx\", 0)\n",
    "            e = _get_field(op, \"ref_end_idx\", 0)\n",
    "            substitutions.extend(ref_words[s:e])\n",
    "\n",
    "    return deletions, insertions, equals, substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01db5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply extract_words_from_alignment to each row in the DataFrame i.e Whisper ASR results and Phi-4 ASR results \n",
    "\n",
    "asr_rows = ['norm_whisper_asr', 'norm_phi4_asr', 'norm_parakeet', 'norm_granite']\n",
    "align_ops_rows = ['norm_whisper_asr_ops', 'norm_phi4_asr_ops', 'norm_parakeet_ops', 'norm_granite_ops']\n",
    "\n",
    "for asr_col, ops_col in zip(asr_rows, align_ops_rows):\n",
    "    all_deletions = []\n",
    "    all_insertions = []\n",
    "    all_equals = []\n",
    "    all_substitutions = []\n",
    "    error_messages = []\n",
    "\n",
    "    for index, row in evaluate_data.iterrows():\n",
    "        try:\n",
    "            result = extract_words_from_alignment(row['norm_human_transcript'], row[asr_col], row[ops_col])\n",
    "            all_deletions.append(result[0])\n",
    "            all_insertions.append(result[1])\n",
    "            all_equals.append(result[2])\n",
    "            all_substitutions.append(result[3])\n",
    "            error_messages.append(\"\")\n",
    "        except Exception as e:\n",
    "            all_deletions.append([])\n",
    "            all_insertions.append([])\n",
    "            all_equals.append([])\n",
    "            all_substitutions.append([])\n",
    "            error_messages.append(str(e))\n",
    "            print(f\"Error processing row {index} for {asr_col}: {str(e)}\")\n",
    "\n",
    "    # Creating DataFrame for the extracted words and error messages\n",
    "    extracted_words_df = pd.DataFrame({\n",
    "        f'{asr_col}_Deletions': all_deletions,\n",
    "        f'{asr_col}_Insertions': all_insertions,\n",
    "        # f'{asr_col}_Equals': all_equals,\n",
    "        f'{asr_col}_Substitutions': all_substitutions,\n",
    "        # f'{asr_col}_Error_Message': error_messages\n",
    "    })\n",
    "\n",
    "    # Concatenate the extracted words DataFrame with the original DataFrame\n",
    "    evaluate_data = pd.concat([evaluate_data, extracted_words_df], axis=1, join='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eceb8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply alignment function to primock as well\n",
    "primock_asr_rows = ['norm_whisper_doctor', 'norm_whisper_patient', 'norm_phi4_doctor', 'norm_phi4_patient',\n",
    "                    'norm_parakeet_doctor', 'norm_parakeet_patient', 'norm_granite_doctor', 'norm_granite_patient']\n",
    "primock_align_ops_rows = ['norm_whisper_doctor_ops', 'norm_whisper_patient_ops', 'norm_phi4_doctor_ops', 'norm_phi4_patient_ops', 'norm_parakeet_doctor_ops', 'norm_parakeet_patient_ops', 'norm_granite_doctor_ops', 'norm_granite_patient_ops']\n",
    "for asr_col, ops_col in zip(primock_asr_rows, primock_align_ops_rows):\n",
    "    all_deletions = []\n",
    "    all_insertions = []\n",
    "    all_equals = []\n",
    "    all_substitutions = []\n",
    "    error_messages = []\n",
    "\n",
    "    for index, row in primock_merged.iterrows():\n",
    "        try:\n",
    "            result = extract_words_from_alignment(\n",
    "                row['norm_human_doctor'] if 'doctor' in asr_col else row['norm_human_patient'],\n",
    "                row[asr_col],\n",
    "                row[ops_col]\n",
    "            )\n",
    "            all_deletions.append(result[0])\n",
    "            all_insertions.append(result[1])\n",
    "            all_equals.append(result[2])\n",
    "            all_substitutions.append(result[3])\n",
    "            error_messages.append(\"\")\n",
    "        except Exception as e:\n",
    "            all_deletions.append([])\n",
    "            all_insertions.append([])\n",
    "            all_equals.append([])\n",
    "            all_substitutions.append([])\n",
    "            error_messages.append(str(e))\n",
    "            print(f\"Error processing row {index} for {asr_col}: {str(e)}\")\n",
    "\n",
    "    # Creating DataFrame for the extracted words and error messages\n",
    "    extracted_words_df = pd.DataFrame({\n",
    "        f'{asr_col}_Deletions': all_deletions,\n",
    "        f'{asr_col}_Insertions': all_insertions,\n",
    "        # f'{asr_col}_Equals': all_equals,\n",
    "        f'{asr_col}_Substitutions': all_substitutions,\n",
    "        # f'{asr_col}_Error_Message': error_messages\n",
    "    })\n",
    "\n",
    "    # Concatenate the extracted words DataFrame with the original DataFrame\n",
    "    primock_merged = pd.concat([primock_merged, extracted_words_df], axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d4850",
   "metadata": {},
   "source": [
    "## Reconstruct Human Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35703fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reconstruct human transcript based on ASR outputs and alignment ops; color-code the errors encountered\n",
    "def reconstruct_human_transcript(hyp_text, alignment_ops):\n",
    "    reconstructed = []\n",
    "    hyp_words = hyp_text.split() if isinstance(hyp_text, str) else []\n",
    "\n",
    "    # if hypothesis empty -> return empty\n",
    "    if not hyp_words:\n",
    "        return \"\"\n",
    "\n",
    "    processed = preprocess_alignment_ops(alignment_ops)\n",
    "\n",
    "    # produce a flat list of op entries regardless of input shape\n",
    "    if isinstance(processed, list):\n",
    "        # flatten one level (handles [[...]] case)\n",
    "        flat = []\n",
    "        for item in processed:\n",
    "            if isinstance(item, list):\n",
    "                flat.extend(item)\n",
    "            else:\n",
    "                flat.append(item)\n",
    "        alignment_ops_list = flat\n",
    "    elif isinstance(processed, dict):\n",
    "        alignment_ops_list = [processed]\n",
    "    else:\n",
    "        if not processed:\n",
    "            return \"\"\n",
    "        try:\n",
    "            alignment_ops_list = ast.literal_eval(processed)\n",
    "            # if parsed to nested list, flatten one level\n",
    "            if isinstance(alignment_ops_list, list) and any(isinstance(x, list) for x in alignment_ops_list):\n",
    "                flat = []\n",
    "                for item in alignment_ops_list:\n",
    "                    if isinstance(item, list):\n",
    "                        flat.extend(item)\n",
    "                    else:\n",
    "                        flat.append(item)\n",
    "                alignment_ops_list = flat\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse alignment_ops: {e}\")\n",
    "\n",
    "    for op in alignment_ops_list:\n",
    "        typ = _get_field(op, \"type\")\n",
    "        if typ == \"delete\":\n",
    "            s = _get_field(op, \"ref_start_idx\", 0)\n",
    "            e = _get_field(op, \"ref_end_idx\", 0)\n",
    "            deleted_words = f\"[DEL: {' '.join(['__'+w+'__' for w in hyp_words[s:e]])}]\"\n",
    "            reconstructed.append(deleted_words)\n",
    "        elif typ == \"equal\":\n",
    "            s = _get_field(op, \"hyp_start_idx\", 0)\n",
    "            e = _get_field(op, \"hyp_end_idx\", 0)\n",
    "            reconstructed.extend(hyp_words[s:e])\n",
    "        elif typ == \"insert\":\n",
    "            s = _get_field(op, \"hyp_start_idx\", 0)\n",
    "            e = _get_field(op, \"hyp_end_idx\", 0)\n",
    "            inserted_words = f\"[INS: {' '.join(['++'+w+'++' for w in hyp_words[s:e]])}]\"\n",
    "            reconstructed.append(inserted_words)\n",
    "        elif typ == \"substitute\":\n",
    "            s = _get_field(op, \"ref_start_idx\", 0)\n",
    "            \n",
    "            e = _get_field(op, \"ref_end_idx\", 0)\n",
    "            substituted_words = f\"[SUB: {' '.join(['~~'+w+'~~' for w in hyp_words[s:e]])}]\"\n",
    "            reconstructed.append(substituted_words)\n",
    "    return ' '.join(reconstructed)\n",
    "\n",
    "# apply reconstruct_human_transcript to each ASR column\n",
    "for asr_col, ops_col in zip(asr_rows, align_ops_rows):\n",
    "    evaluate_data[f'{asr_col}_reconstructed_human'] = evaluate_data.apply(\n",
    "        lambda row: reconstruct_human_transcript(row['norm_human_transcript'], row[ops_col]), axis=1\n",
    "    )\n",
    "\n",
    "# # apply to whipser ASR only for now\n",
    "# evaluate_data['norm_whisper_asr_reconstructed_human'] = evaluate_data.apply(\n",
    "#     lambda row: reconstruct_human_transcript(row['norm_human_transcript'], row['norm_whisper_asr_ops']), axis=1\n",
    "# )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4756f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct human transcript for primock as well\n",
    "primock_asr_rows = ['norm_whisper_doctor', 'norm_whisper_patient', 'norm_phi4_doctor', 'norm_phi4_patient',\n",
    "                    'norm_parakeet_doctor', 'norm_parakeet_patient', 'norm_granite_doctor', 'norm_granite_patient']\n",
    "primock_align_ops_rows = ['norm_whisper_doctor_ops', 'norm_whisper_patient_ops', 'norm_phi4_doctor_ops', 'norm_phi4_patient_ops', 'norm_parakeet_doctor_ops', 'norm_parakeet_patient_ops', 'norm_granite_doctor_ops', 'norm_granite_patient_ops']\n",
    "for asr_col, ops_col in zip(primock_asr_rows, primock_align_ops_rows):\n",
    "    primock_merged[f'{asr_col}_reconstructed_human'] = primock_merged.apply(\n",
    "        lambda row: reconstruct_human_transcript(\n",
    "            row['norm_human_doctor'] if 'doctor' in asr_col else row['norm_human_patient'],\n",
    "            row[ops_col]\n",
    "        ), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d5fa579",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (868003116.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    P: Hi, so I just came in today because I've been just been feeling um, like my head's just been hurting for the past three or four days and I also felt really cold throughout the day for the last few days as well, so, um, I did end up checking my temperature just yesterday afternoon and it was um, 38.7 degrees celsius.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "D: How can I uh, help you today?\n",
    "\n",
    "P: Hi, so I just came in today because I've been just been feeling um, like my head's just been hurting for the past three or four days and I also felt really cold throughout the day for the last few days as well, so, um, I did end up checking my temperature just yesterday afternoon and it was um, 38.7 degrees celsius.\n",
    "\n",
    "D: Okay, um. Tell me about the headache, where exactly is it? \n",
    "\n",
    "P: Yeah so it's kind of just throughout like the front of my forehead and it kind of goes to the side into my temple. \n",
    "\n",
    "D: And what does it feel like?\n",
    "\n",
    "P: It feels like a constant aching. I've had uh, migraines before, but they like, it's not like the pulsating sensation that I usually get with them.\n",
    "\n",
    "D: I see. Uh, okay, and how severe is the pain, if you had to rate it from 1 to 10?\n",
    "\n",
    "P: Um, more than like severe. It's definitely less severe than my migraines, but it, it's just constant. It's, it's just been there for the last four days and I would say probably a 6 out of 10.\n",
    "\n",
    "D: Did you do anything for the pain? Did you take any medications? Anything that makes it better?\n",
    "\n",
    "P: Yeah, I took a Tylenol, I took um, Advil. It helps for like a few hours and then it just comes back.\n",
    "\n",
    "D: And anything that makes the pain worse?\n",
    "\n",
    "P: Um, I definitely think just, if I rest, it's better if, but if I'm like, like I can't, I can't concentrate on work or like anything.\n",
    "\n",
    "D: Okay. Um, and other than the, um, the fever and headaches, have you um, had any um, cough?\n",
    "\n",
    "P: No, no cough. \n",
    "\n",
    "D: Runny nose or congestion?\n",
    "\n",
    "P: No.\n",
    "\n",
    "D: Sore throat?\n",
    "\n",
    "P: No, not really.\n",
    "\n",
    "D: Changes in your sense of smell?\n",
    "\n",
    "P: Uh, I do actually feel like I, like I am losing my smell today.\n",
    "\n",
    "D: And sense of taste?\n",
    "\n",
    "P: I just had breakfast today so um, breakfast was okay. I didn't, uh, but yeah, definitely sense of smell.\n",
    "\n",
    "D: Okay, and any difficulties breathing?\n",
    "\n",
    "P: No.\n",
    "\n",
    "D: Okay, any chest pain?\n",
    "\n",
    "P: No.\n",
    "\n",
    "D: Um, do you feel like your heart is racing?\n",
    "\n",
    "P: No. \n",
    "\n",
    "D: Any diarrhea or constipation?\n",
    "\n",
    "P: No.\n",
    "\n",
    "D: Nausea or vomiting?\n",
    "\n",
    "P: No.\n",
    "\n",
    "D: Pain in your belly?\n",
    "\n",
    "P: No. \n",
    "\n",
    "D: Any muscle aches or joint pains?\n",
    "\n",
    "P: Uh, no, nothing like that.\n",
    "\n",
    "D: Okay, um, do you feel like your neck is stiff?\n",
    "\n",
    "P: No. \n",
    "\n",
    "D: Any numbness or tingling in your arms or fingers, or toes?\n",
    "\n",
    "P: No. \n",
    "\n",
    "D: Any um, night sweats or weight loss, unintentional?\n",
    "\n",
    "P: No, I've only been gaining weight.\n",
    "\n",
    "D: Okay. Any fluid in your, any sort of, swelling in your legs?\n",
    "\n",
    "P: No, nothing like that really.\n",
    "\n",
    "D: Okay, um, any medical conditions that you've been diagnosed with?\n",
    "\n",
    "P: Yes, so I have uh, type 2 diabetes. I was just recently started on metformin medication.\n",
    "\n",
    "D: When were you started on metformin?\n",
    "\n",
    "P: So I think around uh, it's been around two months.\n",
    "\n",
    "D: Okay, how is that going for you? Are you tolerating it okay?\n",
    "\n",
    "P: Yeah I've, I've been tolerating it well. It's just they, they said like I'm kind of just on the edge of the diabetes, like my level, so they just want to act, um, proactively.\n",
    "\n",
    "D: Yeah, that's good. Um, any other medical conditions?\n",
    "\n",
    "P: No, that's, that's all.\n",
    "\n",
    "D: Any medications other than metformin that you take? Anything over the counter?\n",
    "\n",
    "P: Um, so I do take, when my migraines get bad, like I do take, I have taken naproxen before. \n",
    "\n",
    "D: Okay. Surgeries?\n",
    "\n",
    "P: Uh, no surgeries.\n",
    "\n",
    "D: Hospitalizations?\n",
    "\n",
    "P: Uh no.\n",
    "\n",
    "D: Okay, any allergies to medications, foods, or any environmental allergies?\n",
    "\n",
    "P: Not to any medications, but I do have an allergy to peanuts.\n",
    "\n",
    "D: Okay, what happens? \n",
    "\n",
    "P: I do get like a, an anaphylactic reaction. \n",
    "\n",
    "D: Okay, okay. Um, and any um, family history of any medical conditions?\n",
    "\n",
    "P: Uh, so on my dad's side, they like all, like all the men seem to have diabetes, so it's good that I'm starting on the medication. Um on my mom's side, they're pretty healthy. I don't think my mom has any medical conditions.\n",
    "\n",
    "D: Okay, good. Um, and do you currently live alone or do you live with other people?\n",
    "\n",
    "P: Yeah, so I live with my wife, I live with my two kids, a son and daughter.\n",
    "\n",
    "D: And um, have you or any of them had any exposures to anyone who could possibly be sick?\n",
    "\n",
    "P: Um just, uh, so my wife is a school teacher, um, and they did have in-person. Just uh, she's a high school teacher, and they had some they have in-person classes. So yeah, I don't know, but she's been feeling well. My kids are feeling well too.\n",
    "\n",
    "D: Okay, and have you traveled anywhere outside the province?\n",
    "\n",
    "P: No.\n",
    "\n",
    "D: Okay, and do you currently smoke or did you ever smoke in the past?\n",
    "\n",
    "P: No, I've never smoked. I do smoke marijuana just, maybe once or twice a month with some friends.\n",
    "\n",
    "D: Okay, any recreational drugs? \n",
    "\n",
    "P: Sorry, no. \n",
    "\n",
    "D: Um, any alcohol?\n",
    "\n",
    "P: Yeah, I probably drink um, a couple of beers during the weekend.\n",
    "\n",
    "D: Okay, got it. Um, alright. Well, given your symptoms, um, we would like to do a COVID swab, just in, or request a COVID swab to be done through public health, um, just to, just to rule it out. Your symptoms do sound like, or overlap with some of the symptoms that are seen in patients who do have COVID. It could be that you have um, just um, just I guess like your headache and fever. Um, I'll also like do a physical exam to see there's nothing else going on. But in the meantime, I, I'd like you to self isolate and stay away from the rest of your family members as well, um, at least until the results are out. And then when the results are out, if it's positive, you'll receive more guidelines from public health as to what you need to do.\n",
    "\n",
    "P: Okay, sounds good, um, and I work from home, so I don't need to.\n",
    "\n",
    "D: Oh that's good. Okay, so you work from home. \n",
    "\n",
    "P: Alright.\n",
    "\n",
    "D: Good.\n",
    "\n",
    "P: Thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "how can i [DEL: __uh__] uh help you [DEL: __hi__] today hi so i just came [DEL: __because__] in today because [DEL: __been__] ive [DEL: __um__] been just been feeling um like my heads just been hurting for the past three or four days and i also felt really cold throughout the day for the last [DEL: __um__] few days as well so um i did end up [DEL: __and__] checking my [DEL: __um__] temperature just yesterday afternoon and [INS: ++it++] was um 387 degrees celsius okay um tell me about the headache where exactly is [INS: ++it++] yeah so its kind of just throughout like the front of my forehead and it kind of [SUB: ~~temple~~] to the side into my temple and what does it feel like it feels [DEL: __uh__] like a constant aching ive had uh migraines before but they like its not like the pulsating sensation that [DEL: __uh__] i usually get with them i see uh okay and how [SUB: ~~rate~~] is the [SUB: ~~1~~] if [SUB: ~~10~~] had to rate it from 1 to 10 um more than like severe [DEL: __it__] its definitely less severe than my migraines but it its just constant its [SUB: ~~and~~] [DEL: __i__ __would__] just been [SUB: ~~a~~ ~~6~~] the last [DEL: __10__] four days [INS: ++and++ ++i++] would say probably a 6 [SUB: ~~did~~] [DEL: __you__] of 10 did you do anything for the pain did you take any medications anything [SUB: ~~um~~] makes [INS: ++it++] better yeah i took a tylenol i took um advil it helps for like a few hours and then it just comes back and anything that makes the pain worse [DEL: __if__] um i definitely think just if i rest its better if but if im [DEL: __anything__] like like i [INS: ++cant++] i cant concentrate on [INS: ++work++] or like anything [INS: ++okay++] um and other than the um the fever and headaches have you um had any um cough no no cough runny nose or congestion no sore throat [SUB: ~~uh~~] not [INS: ++really++] changes in your sense of [INS: ++smell++] uh i do [SUB: ~~losing~~] feel [SUB: ~~smell~~] i like [INS: ++i++] am losing my [INS: ++smell++] today and sense of taste [INS: ++i++] just [DEL: __um__] had breakfast today so um [SUB: ~~uh~~] was okay [INS: ++i++ ++didnt++] uh [INS: ++but++ ++yeah++] definitely sense of smell okay and any difficulties breathing no okay any chest pain [DEL: __um__] no um do you feel like your heart is racing no any diarrhea or constipation no nausea or vomiting no pain in your belly no any muscle aches or [DEL: __uh__] joint pains uh no nothing [DEL: __um__] like that okay um do you feel like your neck is stiff no any numbness or tingling in your arms or fingers [DEL: __um__] or toes no any um night sweats or [SUB: ~~only~~] loss unintentional no ive only been gaining weight okay any fluid in your any sort of [SUB: ~~nothing~~ ~~like~~ ~~that~~ ~~really~~] no [SUB: ~~um~~] like that really okay um any medical conditions that [DEL: __so__] youve been [DEL: __uh__] diagnosed with yes so i have uh type 2 diabetes i was [SUB: ~~were~~] recently [SUB: ~~started~~] on metformin medication when were you [DEL: __uh__] started on metformin so i think [SUB: ~~how~~] [DEL: __is__] uh its been around two months okay how is that going [DEL: __ive__] for you are you tolerating it okay [DEL: __they__] yeah ive ive been tolerating it well its just they they said like im kind of just on the edge of [DEL: __um__] the diabetes [INS: ++like++] my level so they just want to act um proactively yeah [INS: ++thats++] good [INS: ++um++] any other medical conditions no thats thats all any medications other than metformin that you take [INS: ++anything++] over the counter um so [SUB: ~~like~~] do take when my migraines get [INS: ++bad++] [SUB: ~~naproxen~~] i do [INS: ++take++] i [DEL: __uh__] have taken naproxen [DEL: __uh__] before okay surgeries uh no surgeries hospitalizations uh no okay any allergies to medications foods or any environmental allergies not to any medications but i do have an allergy to [DEL: __a__] peanuts okay what happens i [DEL: __um__] do get [DEL: __um__] like a an anaphylactic reaction okay [DEL: __uh__] okay um and any um [DEL: __they__ __like__] family [DEL: __like__ __all__] history of any medical conditions uh so on my dads side they like all like [DEL: __um__] all the men seem to have diabetes so its good that im starting on the medication [SUB: ~~okay~~] on [DEL: __um__] my moms side theyre pretty healthy i dont think my mom has any [SUB: ~~yeah~~] [DEL: __so__] conditions okay good um and do you currently live alone or do you live with other [DEL: __um__] people yeah so i live with my wife i live with my two kids a son [DEL: __um__] and [DEL: __uh__] daughter and um have you or any [DEL: __um__] of them had any exposures to [DEL: __uh__] anyone who could possibly be sick um just uh so my wife is a school teacher um and they did have inperson just uh shes a high school teacher and they had some they have inperson classes so yeah i dont know but shes been feeling well my kids are feeling well too okay and have you traveled anywhere outside the province no okay and do you currently smoke [SUB: ~~some~~] did you ever smoke in the past [DEL: __um__] no ive never [DEL: __i__] smoked [SUB: ~~drink~~] [DEL: __um__] do smoke [DEL: __of__] marijuana just maybe once or twice a [INS: ++month++ ++with++ ++some++] friends [SUB: ~~alright~~ ~~well~~ ~~given~~] drugs sorry no um any alcohol yeah i [INS: ++probably++] drink [SUB: ~~covid~~] a couple [SUB: ~~in~~] beers during the [SUB: ~~covid~~ ~~swab~~] [DEL: __to__] got it um alright [SUB: ~~health~~] given [INS: ++your++] symptoms um we would like to do [INS: ++a++] covid swab just in or request a covid swab to be done through public health um just to just to rule it out your symptoms do sound [DEL: __um__] like [DEL: __um__] or overlap with some of the symptoms that [DEL: __um__] are seen in patients who do have covid it could be that you have um just um just [DEL: __i__] i guess like your headache and fever um ill also like do a physical exam to see theres [DEL: __um__] nothing else going on but in the meantime i id like you to self isolate and stay away from the rest of your family members as well um at least until the results are [DEL: __um__] out and then when the results are out if its positive youll receive more [SUB: ~~so~~] from public health as [DEL: __alright__] to what you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9deebf7",
   "metadata": {},
   "source": [
    "## Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b08e8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a new excel file\n",
    "evaluate_data.to_excel('all_result_processed.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "957b4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the excel file and display the first few rows\n",
    "df = pd.read_excel('all_result_processed.xlsx', engine='openpyxl')\n",
    "\n",
    "# move each models' results to separate sheets in the excel file\n",
    "with pd.ExcelWriter('all_result_separate_sheets.xlsx', engine='openpyxl') as writer:\n",
    "    whisper_cols = ['utterance_id', 'human-transcript', 'Whisper-ASR', 'norm_human_transcript', 'norm_whisper_asr', \n",
    "                    'norm_whisper_asr_wer', 'norm_whisper_asr_ins', 'norm_whisper_asr_del', \n",
    "                    'norm_whisper_asr_sub', 'norm_whisper_asr_ops',\n",
    "                    'norm_whisper_asr_Deletions', 'norm_whisper_asr_Insertions', 'norm_whisper_asr_Substitutions', 'norm_whisper_asr_reconstructed_human']\n",
    "    phi4_cols = ['utterance_id', 'human-transcript', 'Phi-4-ASR', 'norm_human_transcript', 'norm_phi4_asr', \n",
    "                 'norm_phi4_asr_wer', 'norm_phi4_asr_ins', 'norm_phi4_asr_del', \n",
    "                 'norm_phi4_asr_sub', 'norm_phi4_asr_ops',\n",
    "                 'norm_phi4_asr_Deletions', 'norm_phi4_asr_Insertions', 'norm_phi4_asr_Substitutions', 'norm_phi4_asr_reconstructed_human']\n",
    "    parakeet_cols = ['utterance_id', 'human-transcript', 'Nvidia-Parakeet', 'norm_human_transcript', 'norm_parakeet', \n",
    "                     'norm_parakeet_wer', 'norm_parakeet_ins', 'norm_parakeet_del', \n",
    "                     'norm_parakeet_sub', 'norm_parakeet_ops',\n",
    "                     'norm_parakeet_Deletions', 'norm_parakeet_Insertions', 'norm_parakeet_Substitutions', 'norm_parakeet_reconstructed_human']\n",
    "    granite_cols = ['utterance_id', 'human-transcript', 'IBM-Granite', 'norm_human_transcript', 'norm_granite', \n",
    "                    'norm_granite_wer', 'norm_granite_ins', 'norm_granite_del', \n",
    "                    'norm_granite_sub', 'norm_granite_ops',\n",
    "                    'norm_granite_Deletions', 'norm_granite_Insertions', 'norm_granite_Substitutions', 'norm_granite_reconstructed_human']\n",
    "    df_whisper = df.filter(items=whisper_cols, axis=1)\n",
    "    df_phi4 = df.filter(items=phi4_cols, axis=1)\n",
    "    df_parakeet = df.filter(items=parakeet_cols, axis=1)\n",
    "    df_granite = df.filter(items=granite_cols, axis=1)\n",
    "\n",
    "    # Only write non-empty DataFrames to avoid invisible sheet error\n",
    "    if not df_whisper.empty:\n",
    "        df_whisper.to_excel(writer, sheet_name='Whisper-ASR Results', index=False)\n",
    "    if not df_phi4.empty:\n",
    "        df_phi4.to_excel(writer, sheet_name='Phi-4-ASR Results', index=False)\n",
    "    if not df_parakeet.empty:\n",
    "        df_parakeet.to_excel(writer, sheet_name='Nvidia-Parakeet Results', index=False)\n",
    "    if not df_granite.empty:\n",
    "        df_granite.to_excel(writer, sheet_name='IBM-Granite Results', index=False)\n",
    "        \n",
    "    # resize each row in the sheets to 120px\n",
    "    for sheet_name in ['Whisper-ASR Results', 'Phi-4-ASR Results', 'Nvidia-Parakeet Results', 'IBM-Granite Results']:\n",
    "        worksheet = writer.sheets.get(sheet_name)\n",
    "        if worksheet:\n",
    "            for row_idx in range(1, len(df) + 2):  # +2 to account for header row and 1-based indexing\n",
    "                worksheet.row_dimensions[row_idx].height = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98b28836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate primock results into separate sheets as well\n",
    "with pd.ExcelWriter('primock_result_separate_sheets.xlsx', engine='openpyxl') as writer:\n",
    "    whisper_cols = ['conversation_id', 'turns', 'doctor_utterances', 'patient_utterances', 'norm_human_doctor', 'norm_human_patient', 'norm_whisper_doctor', \n",
    "                    'norm_whisper_patient', 'norm_whisper_doctor_wer', 'norm_whisper_doctor_ins', 'norm_whisper_doctor_del', \n",
    "                    'norm_whisper_doctor_sub', 'norm_whisper_doctor_ops',\n",
    "                    'norm_whisper_doctor_Deletions', 'norm_whisper_doctor_Insertions', 'norm_whisper_doctor_Substitutions', 'norm_whisper_doctor_reconstructed_human',\n",
    "                    'norm_whisper_patient_wer', 'norm_whisper_patient_ins', 'norm_whisper_patient_del', \n",
    "                    'norm_whisper_patient_sub', 'norm_whisper_patient_ops',\n",
    "                    'norm_whisper_patient_Deletions', 'norm_whisper_patient_Insertions', 'norm_whisper_patient_Substitutions', 'norm_whisper_patient_reconstructed_human']\n",
    "    phi4_cols = ['conversation_id', 'turns', 'doctor_utterances', 'patient_utterances', 'norm_human_doctor', 'norm_human_patient', 'norm_phi4_doctor', \n",
    "                 'norm_phi4_patient', 'norm_phi4_doctor_wer', 'norm_phi4_doctor_ins', 'norm_phi4_doctor_del', \n",
    "                 'norm_phi4_doctor_sub', 'norm_phi4_doctor_ops',\n",
    "                 'norm_phi4_doctor_Deletions', 'norm_phi4_doctor_Insertions', 'norm_phi4_doctor_Substitutions', 'norm_phi4_doctor_reconstructed_human',\n",
    "                 'norm_phi4_patient_wer', 'norm_phi4_patient_ins', 'norm_phi4_patient_del', \n",
    "                 'norm_phi4_patient_sub', 'norm_phi4_patient_ops',\n",
    "                 'norm_phi4_patient_Deletions', 'norm_phi4_patient_Insertions', 'norm_phi4_patient_Substitutions', 'norm_phi4_patient_reconstructed_human']\n",
    "    parakeet_cols = ['conversation_id', 'turns', 'doctor_utterances', 'patient_utterances', 'norm_human_doctor', 'norm_human_patient', 'norm_parakeet_doctor', \n",
    "                     'norm_parakeet_patient', 'norm_parakeet_doctor_wer', 'norm_parakeet_doctor_ins', 'norm_parakeet_doctor_del', \n",
    "                     'norm_parakeet_doctor_sub', 'norm_parakeet_doctor_ops',\n",
    "                     'norm_parakeet_doctor_Deletions', 'norm_parakeet_doctor_Insertions', 'norm_parakeet_doctor_Substitutions', 'norm_parakeet_doctor_reconstructed_human',\n",
    "                     'norm_parakeet_patient_wer', 'norm_parakeet_patient_ins', 'norm_parakeet_patient_del', \n",
    "                     'norm_parakeet_patient_sub', 'norm_parakeet_patient_ops',\n",
    "                     'norm_parakeet_patient_Deletions', 'norm_parakeet_patient_Insertions', 'norm_parakeet_patient_Substitutions', 'norm_parakeet_patient_reconstructed_human']\n",
    "    granite_cols = ['conversation_id', 'turns', 'doctor_utterances', 'patient_utterances', 'norm_human_doctor', 'norm_human_patient', 'norm_granite_doctor', \n",
    "                    'norm_granite_patient', 'norm_granite_doctor_wer', 'norm_granite_doctor_ins', 'norm_granite_doctor_del', \n",
    "                    'norm_granite_doctor_sub', 'norm_granite_doctor_ops',\n",
    "                    'norm_granite_doctor_Deletions', 'norm_granite_doctor_Insertions', 'norm_granite_doctor_Substitutions', 'norm_granite_doctor_reconstructed_human',\n",
    "                    'norm_granite_patient_wer', 'norm_granite_patient_ins', 'norm_granite_patient_del', \n",
    "                    'norm_granite_patient_sub', 'norm_granite_patient_ops',\n",
    "                    'norm_granite_patient_Deletions', 'norm_granite_patient_Insertions', 'norm_granite_patient_Substitutions', 'norm_granite_patient_reconstructed_human']\n",
    "    df_whisper = primock_merged.filter(items=whisper_cols, axis=1)\n",
    "    df_phi4 = primock_merged.filter(items=phi4_cols, axis=1)\n",
    "    df_parakeet = primock_merged.filter(items=parakeet_cols, axis=1)\n",
    "    df_granite = primock_merged.filter(items=granite_cols, axis=1)\n",
    "    if not df_whisper.empty:\n",
    "        df_whisper.to_excel(writer, sheet_name='Whisper-ASR Results', index=False)\n",
    "    if not df_phi4.empty:\n",
    "        df_phi4.to_excel(writer, sheet_name='Phi-4-ASR Results', index=False)\n",
    "    if not df_parakeet.empty:\n",
    "        df_parakeet.to_excel(writer, sheet_name='Nvidia-Parakeet Results', index=False)\n",
    "    if not df_granite.empty:\n",
    "        df_granite.to_excel(writer, sheet_name='IBM-Granite Results', index=False)\n",
    "    \n",
    "# resize each row in the sheets to 120px\n",
    "    for sheet_name in ['Whisper-ASR Results', 'Phi-4-ASR Results', 'Nvidia-Parakeet Results', 'IBM-Granite Results']:\n",
    "        worksheet = writer.sheets.get(sheet_name)\n",
    "        if worksheet:\n",
    "            for row_idx in range(1, len(primock_merged) + 2):  # +2 to account for header row and 1-based indexing\n",
    "                worksheet.row_dimensions[row_idx].height = 120\n",
    "    \n",
    "    \n",
    "                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi4_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
