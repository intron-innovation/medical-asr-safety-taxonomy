{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f24816b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from jiwer import wer\n",
    "from jiwer import compute_measures\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ccebe",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746792a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kelechi_phi_whisper = pd.read_excel('results/bio_ramp_asr_results.xlsx',engine='openpyxl')\n",
    "ashisa_results_1 = pd.read_csv('results/afrispeech_asr - afrispeech_asr.csv')\n",
    "aisha_results_2 = pd.read_csv('results/us_medical_20_asr - us_medical_20_asr.csv')\n",
    "\n",
    "# concat the two results\n",
    "aisha_granite_parakeet = pd.concat([ashisa_results_1, aisha_results_2], ignore_index=True, sort=False)\n",
    "\n",
    "evaluate_data = pd.merge(kelechi_phi_whisper, aisha_granite_parakeet[['utterance_id', 'Nvidia-Parakeet', 'IBM-Granite']], on='utterance_id', how='inner')\n",
    "evaluate_data['Phi-4-ASR'] = evaluate_data['Phi-4-ASR'].apply(lambda x: pd.NA if isinstance(x, str) and \"ERROR: CUDA out of memory\" in x else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b3d14",
   "metadata": {},
   "source": [
    "## Text Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d34c1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_timestamps(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove timestamps matching patterns like:\n",
    "      - 03:18:98\n",
    "      - 00:00:001\n",
    "    (pattern: 1-2 digits ':' 1-2 digits ':' 1-3 digits)\n",
    "\n",
    "    Returns cleaned string with extra spaces collapsed.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # remove timestamp tokens\n",
    "    cleaned = re.sub(r'\\b\\d{1,2}:\\d{1,2}:\\d{1,3}\\b', '', text)\n",
    "    # remove new line characters (convert to spaces), then collapse multiple spaces and trim\n",
    "    cleaned = cleaned.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    \n",
    "    # remove speaker tags like: [Speaker 1]:  Speaker 1:  speaker1:  D:  P:\n",
    "    cleaned = re.sub(r'\\[?[Ss]peaker\\s*\\d+\\]?:', '', cleaned)\n",
    "    cleaned = re.sub(r'\\b[Dd]:', '', cleaned)\n",
    "    cleaned = re.sub(r'\\b[Pp]:', '', cleaned)\n",
    "    \n",
    "    # remove all characters that is not A-Z, a-z, 0-9 or space\n",
    "    cleaned = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", cleaned)\n",
    "    \n",
    "    # lowercase the text\n",
    "    cleaned = cleaned.lower()\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91277d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply remove_timestamps to the human transcript column\n",
    "evaluate_data['norm_human_transcript'] = evaluate_data['human-transcript'].apply(remove_timestamps)\n",
    "evaluate_data['norm_whisper_asr'] = evaluate_data['Whisper-ASR'].apply(remove_timestamps)\n",
    "evaluate_data['norm_phi4_asr'] = evaluate_data['Phi-4-ASR'].apply(remove_timestamps)\n",
    "evaluate_data['norm_parakeet'] = evaluate_data['Nvidia-Parakeet'].apply(remove_timestamps)\n",
    "evaluate_data['norm_granite'] = evaluate_data['IBM-Granite'].apply(remove_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040741f",
   "metadata": {},
   "source": [
    "## Calculate WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf63f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_columns = ['norm_whisper_asr', 'norm_phi4_asr', 'norm_parakeet', 'norm_granite']\n",
    "# Calculate WER for each ASR column\n",
    "for norm_col in norm_columns:\n",
    "    evaluate_data[f'{norm_col}_wer_compute'] = evaluate_data.apply(\n",
    "        lambda row: compute_measures(\n",
    "            row['norm_human_transcript'] if pd.notna(row['norm_human_transcript']) else \"\",\n",
    "            row[norm_col] if pd.notna(row[norm_col]) else \"\"\n",
    "        ), axis=1\n",
    "    )\n",
    "    evaluate_data[f'{norm_col}_wer'] = evaluate_data[f'{norm_col}_wer_compute'].apply(lambda measures: measures['wer'])\n",
    "    evaluate_data[f'{norm_col}_ins'] = evaluate_data[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['insertions']\n",
    "    )\n",
    "    evaluate_data[f'{norm_col}_del'] = evaluate_data[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['deletions']\n",
    "    )\n",
    "    evaluate_data[f'{norm_col}_sub'] = evaluate_data[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['substitutions']\n",
    "    )\n",
    "    evaluate_data[f'{norm_col}_ops'] = evaluate_data[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['ops']\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e4ae2",
   "metadata": {},
   "source": [
    "## Add Alignment for Sub, Del and Ins Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c7340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_alignment_ops(alignment_ops):\n",
    "    # If already a list/dict, return as-is (may be nested list)\n",
    "    if isinstance(alignment_ops, (list, dict)):\n",
    "        return alignment_ops\n",
    "    if alignment_ops is None or (isinstance(alignment_ops, float) and pd.isna(alignment_ops)):\n",
    "        return []\n",
    "    s = str(alignment_ops)\n",
    "    s = s.replace(\"AlignmentChunk(\", \"{\").replace(\")\", \"}\").replace(\"type=\", \"'type':\")\\\n",
    "         .replace(\"ref_start_idx=\", \"'ref_start_idx':\").replace(\"ref_end_idx=\", \"'ref_end_idx':\")\\\n",
    "         .replace(\"hyp_start_idx=\", \"'hyp_start_idx':\").replace(\"hyp_end_idx=\", \"'hyp_end_idx':\")\n",
    "    if s.startswith(\"[[\") and s.endswith(\"]]\"):\n",
    "        s = s[1:-1]\n",
    "    return s\n",
    "\n",
    "def _get_field(op, field, default=None):\n",
    "    # support dict-like\n",
    "    if isinstance(op, dict):\n",
    "        return op.get(field, default)\n",
    "    # support object with attribute (AlignmentChunk)\n",
    "    if hasattr(op, field):\n",
    "        return getattr(op, field, default)\n",
    "    # support tuple/list with numeric positions (fallback not used here)\n",
    "    return default\n",
    "\n",
    "def extract_words_from_alignment(ref_text, hyp_text, alignment_ops):\n",
    "    deletions, insertions, substitutions, equals = [], [], [], []\n",
    "    ref_words = ref_text.split() if isinstance(ref_text, str) else []\n",
    "    hyp_words = hyp_text.split() if isinstance(hyp_text, str) else []\n",
    "\n",
    "    # if hypothesis empty -> all deleted\n",
    "    if not hyp_words:\n",
    "        return ref_words, [], [], []\n",
    "\n",
    "    processed = preprocess_alignment_ops(alignment_ops)\n",
    "\n",
    "    # produce a flat list of op entries regardless of input shape\n",
    "    if isinstance(processed, list):\n",
    "        # flatten one level (handles [[...]] case)\n",
    "        flat = []\n",
    "        for item in processed:\n",
    "            if isinstance(item, list):\n",
    "                flat.extend(item)\n",
    "            else:\n",
    "                flat.append(item)\n",
    "        alignment_ops_list = flat\n",
    "    elif isinstance(processed, dict):\n",
    "        alignment_ops_list = [processed]\n",
    "    else:\n",
    "        if not processed:\n",
    "            return deletions, insertions, equals, substitutions\n",
    "        try:\n",
    "            alignment_ops_list = ast.literal_eval(processed)\n",
    "            # if parsed to nested list, flatten one level\n",
    "            if isinstance(alignment_ops_list, list) and any(isinstance(x, list) for x in alignment_ops_list):\n",
    "                flat = []\n",
    "                for item in alignment_ops_list:\n",
    "                    if isinstance(item, list):\n",
    "                        flat.extend(item)\n",
    "                    else:\n",
    "                        flat.append(item)\n",
    "                alignment_ops_list = flat\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse alignment_ops for row: {e}\")\n",
    "\n",
    "    for op in alignment_ops_list:\n",
    "        typ = _get_field(op, \"type\")\n",
    "        if typ == \"delete\":\n",
    "            s = _get_field(op, \"ref_start_idx\", 0)\n",
    "            e = _get_field(op, \"ref_end_idx\", 0)\n",
    "            deletions.extend(ref_words[s:e])\n",
    "        elif typ == \"equal\":\n",
    "            s = _get_field(op, \"hyp_start_idx\", 0)\n",
    "            e = _get_field(op, \"hyp_end_idx\", 0)\n",
    "            equals.extend(hyp_words[s:e])\n",
    "        elif typ == \"insert\":\n",
    "            s = _get_field(op, \"hyp_start_idx\", 0)\n",
    "            e = _get_field(op, \"hyp_end_idx\", 0)\n",
    "            insertions.extend(hyp_words[s:e])\n",
    "        elif typ == \"substitute\":\n",
    "            s = _get_field(op, \"ref_start_idx\", 0)\n",
    "            e = _get_field(op, \"ref_end_idx\", 0)\n",
    "            substitutions.extend(ref_words[s:e])\n",
    "\n",
    "    return deletions, insertions, equals, substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01db5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply extract_words_from_alignment to each row in the DataFrame i.e Whisper ASR results and Phi-4 ASR results \n",
    "\n",
    "asr_rows = ['norm_whisper_asr', 'norm_phi4_asr', 'norm_parakeet', 'norm_granite']\n",
    "align_ops_rows = ['norm_whisper_asr_ops', 'norm_phi4_asr_ops', 'norm_parakeet_ops', 'norm_granite_ops']\n",
    "\n",
    "for asr_col, ops_col in zip(asr_rows, align_ops_rows):\n",
    "    all_deletions = []\n",
    "    all_insertions = []\n",
    "    all_equals = []\n",
    "    all_substitutions = []\n",
    "    error_messages = []\n",
    "\n",
    "    for index, row in evaluate_data.iterrows():\n",
    "        try:\n",
    "            result = extract_words_from_alignment(row['norm_human_transcript'], row[asr_col], row[ops_col])\n",
    "            all_deletions.append(result[0])\n",
    "            all_insertions.append(result[1])\n",
    "            all_equals.append(result[2])\n",
    "            all_substitutions.append(result[3])\n",
    "            error_messages.append(\"\")\n",
    "        except Exception as e:\n",
    "            all_deletions.append([])\n",
    "            all_insertions.append([])\n",
    "            all_equals.append([])\n",
    "            all_substitutions.append([])\n",
    "            error_messages.append(str(e))\n",
    "            print(f\"Error processing row {index} for {asr_col}: {str(e)}\")\n",
    "\n",
    "    # Creating DataFrame for the extracted words and error messages\n",
    "    extracted_words_df = pd.DataFrame({\n",
    "        f'{asr_col}_Deletions': all_deletions,\n",
    "        f'{asr_col}_Insertions': all_insertions,\n",
    "        # f'{asr_col}_Equals': all_equals,\n",
    "        f'{asr_col}_Substitutions': all_substitutions,\n",
    "        # f'{asr_col}_Error_Message': error_messages\n",
    "    })\n",
    "\n",
    "    # Concatenate the extracted words DataFrame with the original DataFrame\n",
    "    evaluate_data = pd.concat([evaluate_data, extracted_words_df], axis=1, join='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a new excel file\n",
    "evaluate_data.to_excel('all_result_processed.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the excel file and display the first few rows\n",
    "df = pd.read_excel('all_result_processed.xlsx', engine='openpyxl')\n",
    "\n",
    "# move each models' results to separate sheets in the excel file\n",
    "with pd.ExcelWriter('all_result_separate_sheets.xlsx', engine='openpyxl') as writer:\n",
    "    whisper_cols = ['utterance_id', 'human-transcript', 'Whisper-ASR', 'norm_whisper_asr', \n",
    "                    'norm_whisper_asr_wer', 'norm_whisper_asr_ins', 'norm_whisper_asr_del', \n",
    "                    'norm_whisper_asr_sub', 'norm_whisper_asr_ops',\n",
    "                    'norm_whisper_asr_Deletions', 'norm_whisper_asr_Insertions', 'norm_whisper_asr_Substitutions']\n",
    "    phi4_cols = ['utterance_id', 'human-transcript', 'Phi-4-ASR', 'norm_phi4_asr', \n",
    "                 'norm_phi4_asr_wer', 'norm_phi4_asr_ins', 'norm_phi4_asr_del', \n",
    "                 'norm_phi4_asr_sub', 'norm_phi4_asr_ops',\n",
    "                 'norm_phi4_asr_Deletions', 'norm_phi4_asr_Insertions', 'norm_phi4_asr_Substitutions']\n",
    "    parakeet_cols = ['utterance_id', 'human-transcript', 'Nvidia-Parakeet', 'norm_parakeet', \n",
    "                     'norm_parakeet_wer', 'norm_parakeet_ins', 'norm_parakeet_del', \n",
    "                     'norm_parakeet_sub', 'norm_parakeet_ops',\n",
    "                     'norm_parakeet_Deletions', 'norm_parakeet_Insertions', 'norm_parakeet_Substitutions']\n",
    "    granite_cols = ['utterance_id', 'human-transcript', 'IBM-Granite', 'norm_granite', \n",
    "                    'norm_granite_wer', 'norm_granite_ins', 'norm_granite_del', \n",
    "                    'norm_granite_sub', 'norm_granite_ops',\n",
    "                    'norm_granite_Deletions', 'norm_granite_Insertions', 'norm_granite_Substitutions']\n",
    "\n",
    "    df_whisper = df.filter(items=whisper_cols, axis=1)\n",
    "    df_phi4 = df.filter(items=phi4_cols, axis=1)\n",
    "    df_parakeet = df.filter(items=parakeet_cols, axis=1)\n",
    "    df_granite = df.filter(items=granite_cols, axis=1)\n",
    "\n",
    "    # Only write non-empty DataFrames to avoid invisible sheet error\n",
    "    if not df_whisper.empty:\n",
    "        df_whisper.to_excel(writer, sheet_name='Whisper-ASR Results', index=False)\n",
    "    if not df_phi4.empty:\n",
    "        df_phi4.to_excel(writer, sheet_name='Phi-4-ASR Results', index=False)\n",
    "    if not df_parakeet.empty:\n",
    "        df_parakeet.to_excel(writer, sheet_name='Nvidia-Parakeet Results', index=False)\n",
    "    if not df_granite.empty:\n",
    "        df_granite.to_excel(writer, sheet_name='IBM-Granite Results', index=False)\n",
    "        \n",
    "    # resize each row in the sheets to 120px\n",
    "    for sheet_name in ['Whisper-ASR Results', 'Phi-4-ASR Results', 'Nvidia-Parakeet Results', 'IBM-Granite Results']:\n",
    "        worksheet = writer.sheets.get(sheet_name)\n",
    "        if worksheet:\n",
    "            for row_idx in range(1, len(df) + 2):  # +2 to account for header row and 1-based indexing\n",
    "                worksheet.row_dimensions[row_idx].height = 120\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi4_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
