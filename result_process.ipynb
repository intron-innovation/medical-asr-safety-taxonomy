{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f24816b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from jiwer import wer\n",
    "from jiwer import compute_measures\n",
    "import ast\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# normalization library\n",
    "import unicodedata\n",
    "import contractions\n",
    "from num2words import num2words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ccebe",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "746792a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi4_whisper_result = pd.read_csv('results/whisper_phi4_asr_results_all.csv')\n",
    "\n",
    "nvidia_parakeet_results = pd.read_csv('results/nvidia_parakeet_asr_results.csv')\n",
    "# aisha_results_2 = pd.read_csv('results/us_medical_20_asr - us_medical_20_asr.csv')\n",
    "\n",
    "# # concat the two results\n",
    "# aisha_granite_parakeet = pd.concat([ashisa_results_1, aisha_results_2], ignore_index=True, sort=False)\n",
    "\n",
    "evaluate_data = pd.merge(phi4_whisper_result, nvidia_parakeet_results[['utterance_id', 'Nvidia-Parakeet-ASR']], on='utterance_id', how='inner')\n",
    "# evaluate_data = phi4_whisper_result\n",
    "# evaluate_data['Phi-4-ASR'] = evaluate_data['Phi-4-ASR'].apply(lambda x: pd.NA if isinstance(x, str) and \"ERROR: CUDA out of memory\" in x else x)\n",
    "## Rename 'transcript' to human transcript\n",
    "evaluate_data = evaluate_data.rename(columns={'transcript': 'human-transcript'})\n",
    "\n",
    "# # Merge Primock results\n",
    "# primock_merged = pd.merge(primock_phi_whisper, primock_granite_parakeet[['utterance_id', 'Nvidia-Parakeet-ASR-doctor', 'Nvidia-Parakeet-ASR-patient', 'IBM-Granite-doctor', 'IBM-Granite-patient']], on='utterance_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b3d14",
   "metadata": {},
   "source": [
    "## Text Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d34c1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_timestamps(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove timestamps matching patterns like:\n",
    "      - 03:18:98\n",
    "      - 00:00:001\n",
    "    (pattern: 1-2 digits ':' 1-2 digits ':' 1-3 digits)\n",
    "\n",
    "    Returns cleaned string with extra spaces collapsed.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # remove timestamp tokens\n",
    "    cleaned = re.sub(r'\\b\\d{1,2}:\\d{1,2}:\\d{1,3}\\b', '', text)\n",
    "    # remove new line characters (convert to spaces), then collapse multiple spaces and trim\n",
    "    cleaned = cleaned.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    \n",
    "    # remove speaker tags like: [Speaker 1]:  Speaker 1:  speaker1:  D:  P:\n",
    "    cleaned = re.sub(r'\\[?[Ss]peaker\\s*\\d+\\]?:', '', cleaned)\n",
    "    cleaned = re.sub(r'\\b[Dd]:', '', cleaned)\n",
    "    cleaned = re.sub(r'\\b[Pp]:', '', cleaned)\n",
    "    cleaned = re.sub(r'\\bDOCTOR:\\s*', '', cleaned)\n",
    "    cleaned = re.sub(r'\\bPATIENT:\\s*', '', cleaned)\n",
    "    \n",
    "    # remove tags such <INAUDIBLE_SPEECH/>, <UNSURE> <UNSURE/>,  <UNIN/> etc\n",
    "    cleaned = re.sub(r'<[^>]+>', '', cleaned)\n",
    "    \n",
    "    # expand contractions\n",
    "    cleaned = contractions.fix(cleaned)\n",
    "    \n",
    "    # convert numbers to words using num2words\n",
    "    cleaned = re.sub(r'\\d+', lambda x: num2words(int(x.group())), cleaned)\n",
    "    \n",
    "    # # remove all characters that is not A-Z, a-z, 0-9 or space\n",
    "    cleaned = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", cleaned)\n",
    "    \n",
    "    # lowercase the text\n",
    "    cleaned = cleaned.lower()\n",
    "    \n",
    "    \n",
    "    cleaned = re.sub(r'\\bok\\b', 'okay', cleaned)\n",
    "    cleaned = re.sub(r'\\bohh\\b', 'oh', cleaned)\n",
    "    cleaned = re.sub(r'\\bdr\\b', 'doctor', cleaned)\n",
    "    cleaned = re.sub(r'\\bpt\\b', 'patient', cleaned)\n",
    "    \n",
    "    # cleaned = re.sub(r'\\bokay\\b', 'ok', cleaned)\n",
    "    # cleaned = re.sub(r'\\boh\\b', 'ooh', cleaned)\n",
    "    # cleaned = re.sub(r'\\bdoctor\\b', 'dr', cleaned)\n",
    "    # cleaned = re.sub(r'\\bpatient\\b', 'pt', cleaned)\n",
    "    # remove extra spaces again after replacements\n",
    "    # cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    cleaned = re.sub(r'\\b(um|uh|erm|uhm|mmhmm|ah|umm)\\b', '', cleaned, flags=re.IGNORECASE)\n",
    "    \n",
    "    return cleaned\n",
    "  \n",
    "  \n",
    "# def plain_normalize(text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Normalize text by:\n",
    "#       - Expanding contractions\n",
    "#       - Converting numbers to words\n",
    "#       - Remove disfluencies and timestamps\n",
    "#       - Modify common abbreviations (e.g., \"Dr.\" to \"Doctor\", \"Pt.\" to \"Patient\")\n",
    "#     \"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         return text\n",
    "    \n",
    "#     # expand contractions\n",
    "#     cleaned_text = contractions.fix(text)\n",
    "    \n",
    "#     # remove disfluencies\n",
    "#     cleaned_text = re.sub(r'\\b(um|uh|erm|uhm|mmhmm|ah)\\b', '', cleaned_text, flags=re.IGNORECASE)\n",
    "    \n",
    "#     # common abbreviation replacements\n",
    "#     cleaned_text = re.sub(r'\\bDr\\.\\s*', 'Doctor ', cleaned_text)\n",
    "#     cleaned_text = re.sub(r'\\bPt\\.\\s*', 'Patient ', cleaned_text)\n",
    "#     cleaned_text = re.sub(r'\\bOk\\.\\s*', 'Okay ', cleaned_text)\n",
    "#     cleaned_text = re.sub(r'\\bOhh\\.\\s*', 'Oh ', cleaned_text)\n",
    "    \n",
    "#     return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f22874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse Turn columns to extract Doctor and Patient utterances\n",
    "def extract_utterances(turns):\n",
    "    \"\"\"Extract and format utterances from turns data.\"\"\"\n",
    "    if isinstance(turns, str):\n",
    "        try:\n",
    "            turns = ast.literal_eval(turns)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return ''\n",
    "    if not turns or not hasattr(turns, '__iter__'):\n",
    "        return ''\n",
    "    \n",
    "    formatted_transcript = []\n",
    "    \n",
    "    for turn in turns:\n",
    "        # Ensure turn is a dictionary\n",
    "        if not isinstance(turn, dict):\n",
    "            continue\n",
    "            \n",
    "        speaker = turn.get('speaker', '').upper()\n",
    "        utterance = turn.get('text', '')\n",
    "        \n",
    "        # Format as \"SPEAKER: utterance\"\n",
    "        if 'DOCTOR' in speaker:\n",
    "            formatted_transcript.append(f\"DOCTOR: {utterance}\")\n",
    "        elif 'PATIENT' in speaker:\n",
    "            formatted_transcript.append(f\"PATIENT: {utterance}\")\n",
    "    \n",
    "    return ' '.join(formatted_transcript)\n",
    "\n",
    "# Apply only to UK-Dataset rows\n",
    "uk_mask = evaluate_data['source'] == 'UK-Dataset'\n",
    "evaluate_data.loc[uk_mask, 'human-transcript'] = (evaluate_data.loc[uk_mask, 'human-transcript'].apply(extract_utterances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f91277d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply remove_timestamps to the human transcript column\n",
    "evaluate_data['norm_human_transcript'] = evaluate_data['human-transcript'].apply(remove_timestamps)\n",
    "evaluate_data['norm_whisper_asr'] = evaluate_data['Whisper-ASR'].apply(remove_timestamps)\n",
    "evaluate_data['norm_phi4_asr'] = evaluate_data['Phi-4-ASR'].apply(remove_timestamps)\n",
    "evaluate_data['norm_parakeet_asr'] = evaluate_data['Nvidia-Parakeet-ASR'].apply(remove_timestamps)\n",
    "# evaluate_data['norm_granite'] = evaluate_data['IBM-Granite'].apply(remove_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040741f",
   "metadata": {},
   "source": [
    "## Calculate WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf63f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_columns = ['norm_whisper_asr', 'norm_phi4_asr', 'norm_parakeet_asr'] #'norm_parakeet_asr', 'norm_granite'\n",
    "# Calculate WER for each ASR column\n",
    "for norm_col in norm_columns:\n",
    "    evaluate_data[f'{norm_col}_wer_compute'] = evaluate_data.apply(\n",
    "        lambda row: compute_measures(\n",
    "            row['norm_human_transcript'] if pd.notna(row['norm_human_transcript']) else \"\",\n",
    "            row[norm_col] if pd.notna(row[norm_col]) else \"\"\n",
    "        ), axis=1\n",
    "    )\n",
    "    evaluate_data[f'{norm_col}_wer'] = evaluate_data[f'{norm_col}_wer_compute'].apply(lambda measures: measures['wer'])\n",
    "    evaluate_data[f'{norm_col}_ins'] = evaluate_data[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['insertions']\n",
    "    )\n",
    "    evaluate_data[f'{norm_col}_del'] = evaluate_data[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['deletions']\n",
    "    )\n",
    "    evaluate_data[f'{norm_col}_sub'] = evaluate_data[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['substitutions']\n",
    "    )\n",
    "    evaluate_data[f'{norm_col}_ops'] = evaluate_data[f'{norm_col}_wer_compute'].apply(\n",
    "        lambda measures: measures['ops']\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "094ae141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply wer calculation to primock as well\n",
    "# primock_norm_columns = ['norm_whisper_doctor', 'norm_whisper_patient', 'norm_phi4_doctor', 'norm_phi4_patient',\n",
    "#                         'norm_granite_doctor', 'norm_granite_patient', 'norm_parakeet_doctor', 'norm_parakeet_patient']\n",
    "# for norm_col in primock_norm_columns:\n",
    "#     primock_merged[f'{norm_col}_wer_compute'] = primock_merged.apply(\n",
    "#         lambda row: compute_measures(\n",
    "#             row['norm_human_doctor'] if 'doctor' in norm_col else row['norm_human_patient'],\n",
    "#             row[norm_col] if pd.notna(row[norm_col]) else \"\"\n",
    "#         ), axis=1\n",
    "#     )\n",
    "#     primock_merged[f'{norm_col}_wer'] = primock_merged[f'{norm_col}_wer_compute'].apply(lambda measures: measures['wer'])\n",
    "#     primock_merged[f'{norm_col}_ins'] = primock_merged[f'{norm_col}_wer_compute'].apply(\n",
    "#         lambda measures: measures['insertions']\n",
    "#     )\n",
    "#     primock_merged[f'{norm_col}_del'] = primock_merged[f'{norm_col}_wer_compute'].apply(\n",
    "#         lambda measures: measures['deletions']\n",
    "#     )\n",
    "#     primock_merged[f'{norm_col}_sub'] = primock_merged[f'{norm_col}_wer_compute'].apply(\n",
    "#         lambda measures: measures['substitutions']\n",
    "#     )\n",
    "#     primock_merged[f'{norm_col}_ops'] = primock_merged[f'{norm_col}_wer_compute'].apply(\n",
    "#         lambda measures: measures['ops']\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e4ae2",
   "metadata": {},
   "source": [
    "## Add Alignment for Sub, Del and Ins Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90c7340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_alignment_ops(alignment_ops):\n",
    "    # If already a list/dict, return as-is (may be nested list)\n",
    "    if isinstance(alignment_ops, (list, dict)):\n",
    "        return alignment_ops\n",
    "    if alignment_ops is None or (isinstance(alignment_ops, float) and pd.isna(alignment_ops)):\n",
    "        return []\n",
    "    s = str(alignment_ops)\n",
    "    s = s.replace(\"AlignmentChunk(\", \"{\").replace(\")\", \"}\").replace(\"type=\", \"'type':\")\\\n",
    "         .replace(\"ref_start_idx=\", \"'ref_start_idx':\").replace(\"ref_end_idx=\", \"'ref_end_idx':\")\\\n",
    "         .replace(\"hyp_start_idx=\", \"'hyp_start_idx':\").replace(\"hyp_end_idx=\", \"'hyp_end_idx':\")\n",
    "    if s.startswith(\"[[\") and s.endswith(\"]]\"):\n",
    "        s = s[1:-1]\n",
    "    return s\n",
    "\n",
    "def _get_field(op, field, default=None):\n",
    "    # support dict-like\n",
    "    if isinstance(op, dict):\n",
    "        return op.get(field, default)\n",
    "    # support object with attribute (AlignmentChunk)\n",
    "    if hasattr(op, field):\n",
    "        return getattr(op, field, default)\n",
    "    # support tuple/list with numeric positions (fallback not used here)\n",
    "    return default\n",
    "\n",
    "def extract_words_from_alignment(ref_text, hyp_text, alignment_ops):\n",
    "    deletions, insertions, substitutions, equals = [], [], [], []\n",
    "    ref_words = ref_text.split() if isinstance(ref_text, str) else []\n",
    "    hyp_words = hyp_text.split() if isinstance(hyp_text, str) else []\n",
    "\n",
    "    # if hypothesis empty -> all deleted\n",
    "    if not hyp_words:\n",
    "        return ref_words, [], [], []\n",
    "\n",
    "    processed = preprocess_alignment_ops(alignment_ops)\n",
    "\n",
    "    # produce a flat list of op entries regardless of input shape\n",
    "    if isinstance(processed, list):\n",
    "        # flatten one level (handles [[...]] case)\n",
    "        flat = []\n",
    "        for item in processed:\n",
    "            if isinstance(item, list):\n",
    "                flat.extend(item)\n",
    "            else:\n",
    "                flat.append(item)\n",
    "        alignment_ops_list = flat\n",
    "    elif isinstance(processed, dict):\n",
    "        alignment_ops_list = [processed]\n",
    "    else:\n",
    "        if not processed:\n",
    "            return deletions, insertions, equals, substitutions\n",
    "        try:\n",
    "            alignment_ops_list = ast.literal_eval(processed)\n",
    "            # if parsed to nested list, flatten one level\n",
    "            if isinstance(alignment_ops_list, list) and any(isinstance(x, list) for x in alignment_ops_list):\n",
    "                flat = []\n",
    "                for item in alignment_ops_list:\n",
    "                    if isinstance(item, list):\n",
    "                        flat.extend(item)\n",
    "                    else:\n",
    "                        flat.append(item)\n",
    "                alignment_ops_list = flat\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse alignment_ops for row: {e}\")\n",
    "\n",
    "    for op in alignment_ops_list:\n",
    "        typ = _get_field(op, \"type\")\n",
    "        if typ == \"delete\":\n",
    "            s = _get_field(op, \"ref_start_idx\", 0)\n",
    "            e = _get_field(op, \"ref_end_idx\", 0)\n",
    "            deletions.extend(ref_words[s:e])\n",
    "        elif typ == \"equal\":\n",
    "            s = _get_field(op, \"hyp_start_idx\", 0)\n",
    "            e = _get_field(op, \"hyp_end_idx\", 0)\n",
    "            equals.extend(hyp_words[s:e])\n",
    "        elif typ == \"insert\":\n",
    "            s = _get_field(op, \"hyp_start_idx\", 0)\n",
    "            e = _get_field(op, \"hyp_end_idx\", 0)\n",
    "            insertions.extend(hyp_words[s:e])\n",
    "        elif typ == \"substitute\":\n",
    "            s = _get_field(op, \"ref_start_idx\", 0)\n",
    "            e = _get_field(op, \"ref_end_idx\", 0)\n",
    "            substitutions.extend(ref_words[s:e])\n",
    "\n",
    "    return deletions, insertions, equals, substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01db5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply extract_words_from_alignment to each row in the DataFrame i.e Whisper ASR results and Phi-4 ASR results \n",
    "\n",
    "# asr_rows = ['norm_whisper_asr', 'norm_phi4_asr', 'norm_parakeet_asr', 'norm_granite']\n",
    "asr_rows = ['norm_whisper_asr', 'norm_phi4_asr', 'norm_parakeet_asr']\n",
    "# align_ops_rows = ['norm_whisper_asr_ops', 'norm_phi4_asr_ops', 'norm_parakeet_ops', 'norm_granite_ops']\n",
    "align_ops_rows = ['norm_whisper_asr_ops', 'norm_phi4_asr_ops', 'norm_parakeet_asr_ops']\n",
    "\n",
    "for asr_col, ops_col in zip(asr_rows, align_ops_rows):\n",
    "    all_deletions = []\n",
    "    all_insertions = []\n",
    "    all_equals = []\n",
    "    all_substitutions = []\n",
    "    error_messages = []\n",
    "\n",
    "    for index, row in evaluate_data.iterrows():\n",
    "        try:\n",
    "            result = extract_words_from_alignment(row['norm_human_transcript'], row[asr_col], row[ops_col])\n",
    "            all_deletions.append(result[0])\n",
    "            all_insertions.append(result[1])\n",
    "            all_equals.append(result[2])\n",
    "            all_substitutions.append(result[3])\n",
    "            error_messages.append(\"\")\n",
    "        except Exception as e:\n",
    "            all_deletions.append([])\n",
    "            all_insertions.append([])\n",
    "            all_equals.append([])\n",
    "            all_substitutions.append([])\n",
    "            error_messages.append(str(e))\n",
    "            print(f\"Error processing row {index} for {asr_col}: {str(e)}\")\n",
    "\n",
    "    # Creating DataFrame for the extracted words and error messages\n",
    "    extracted_words_df = pd.DataFrame({\n",
    "        f'{asr_col}_Deletions': all_deletions,\n",
    "        f'{asr_col}_Insertions': all_insertions,\n",
    "        # f'{asr_col}_Equals': all_equals,\n",
    "        f'{asr_col}_Substitutions': all_substitutions,\n",
    "        # f'{asr_col}_Error_Message': error_messages\n",
    "    })\n",
    "\n",
    "    # Concatenate the extracted words DataFrame with the original DataFrame\n",
    "    evaluate_data = pd.concat([evaluate_data, extracted_words_df], axis=1, join='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eceb8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply alignment function to primock as well\n",
    "# primock_asr_rows = ['norm_whisper_doctor', 'norm_whisper_patient', 'norm_phi4_doctor', 'norm_phi4_patient',\n",
    "#                     'norm_parakeet_doctor', 'norm_parakeet_patient', 'norm_granite_doctor', 'norm_granite_patient']\n",
    "# primock_align_ops_rows = ['norm_whisper_doctor_ops', 'norm_whisper_patient_ops', 'norm_phi4_doctor_ops', 'norm_phi4_patient_ops', 'norm_parakeet_doctor_ops', 'norm_parakeet_patient_ops', 'norm_granite_doctor_ops', 'norm_granite_patient_ops']\n",
    "# for asr_col, ops_col in zip(primock_asr_rows, primock_align_ops_rows):\n",
    "#     all_deletions = []\n",
    "#     all_insertions = []\n",
    "#     all_equals = []\n",
    "#     all_substitutions = []\n",
    "#     error_messages = []\n",
    "\n",
    "#     for index, row in primock_merged.iterrows():\n",
    "#         try:\n",
    "#             result = extract_words_from_alignment(\n",
    "#                 row['norm_human_doctor'] if 'doctor' in asr_col else row['norm_human_patient'],\n",
    "#                 row[asr_col],\n",
    "#                 row[ops_col]\n",
    "#             )\n",
    "#             all_deletions.append(result[0])\n",
    "#             all_insertions.append(result[1])\n",
    "#             all_equals.append(result[2])\n",
    "#             all_substitutions.append(result[3])\n",
    "#             error_messages.append(\"\")\n",
    "#         except Exception as e:\n",
    "#             all_deletions.append([])\n",
    "#             all_insertions.append([])\n",
    "#             all_equals.append([])\n",
    "#             all_substitutions.append([])\n",
    "#             error_messages.append(str(e))\n",
    "#             print(f\"Error processing row {index} for {asr_col}: {str(e)}\")\n",
    "\n",
    "#     # Creating DataFrame for the extracted words and error messages\n",
    "#     extracted_words_df = pd.DataFrame({\n",
    "#         f'{asr_col}_Deletions': all_deletions,\n",
    "#         f'{asr_col}_Insertions': all_insertions,\n",
    "#         # f'{asr_col}_Equals': all_equals,\n",
    "#         f'{asr_col}_Substitutions': all_substitutions,\n",
    "#         # f'{asr_col}_Error_Message': error_messages\n",
    "#     })\n",
    "\n",
    "#     # Concatenate the extracted words DataFrame with the original DataFrame\n",
    "#     primock_merged = pd.concat([primock_merged, extracted_words_df], axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d4850",
   "metadata": {},
   "source": [
    "## Reconstruct Human Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32b575ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_words(ref, hyp, ref_range):\n",
    "    if pd.isna(ref) or pd.isna(hyp):\n",
    "        return None\n",
    "    \n",
    "    ref = ref.split()\n",
    "    hyp = hyp.split()\n",
    "    lexicon = list(set(ref + hyp))\n",
    "    word2digit = {word: chr(i) for i, word in enumerate(lexicon)}\n",
    "    ref_uni = [word2digit[w] for w in ref]\n",
    "    hyp_uni = [word2digit[w] for w in hyp]\n",
    "    \n",
    "    edit_ops = pd.DataFrame(Levenshtein.editops(''.join(ref_uni), ''.join(hyp_uni)), columns=['operation', 'ref_ix', 'hyp_ix'])\n",
    "    aligned_ref, aligned_hyp = ref.copy(), hyp.copy()\n",
    "    aligned_ops = ['='] * len(ref)\n",
    "    aligned_ref_ix, aligned_hyp_ix = list(range(len(ref))), list(range(len(hyp)))\n",
    "    ix_edit_ops = [np.NaN] * len(aligned_ref)\n",
    "\n",
    "    ins_count, del_count = 0, 0\n",
    "    for idx, ops in edit_ops.iterrows():\n",
    "        if ops['operation'] == 'insert':\n",
    "            aligned_ref.insert(ins_count + ops['ref_ix'], '_')\n",
    "            aligned_ops.insert(ins_count + ops['ref_ix'], 'ins')\n",
    "            aligned_ref_ix.insert(ins_count + ops['ref_ix'], None)\n",
    "            ix_edit_ops.insert(ins_count + ops['ref_ix'], idx)\n",
    "            ins_count += 1\n",
    "        elif ops['operation'] == 'delete':\n",
    "            aligned_hyp.insert(del_count + ops['hyp_ix'], '_')\n",
    "            aligned_ops[ins_count + ops['ref_ix']] = 'del'\n",
    "            aligned_hyp_ix.insert(del_count + ops['hyp_ix'], None)\n",
    "            ix_edit_ops[ins_count + ops['ref_ix']] = idx\n",
    "            del_count += 1\n",
    "        elif ops['operation'] == 'replace':\n",
    "            aligned_ops[ins_count + ops['ref_ix']] = 'sub'\n",
    "            ix_edit_ops[ins_count + ops['ref_ix']] = idx\n",
    "\n",
    "    aligned_df = pd.DataFrame({\n",
    "        'ref_ix': aligned_ref_ix,\n",
    "        'hyp_ix': aligned_hyp_ix,\n",
    "        'reference': aligned_ref,\n",
    "        'hypothesis': aligned_hyp,\n",
    "        'operation': aligned_ops,\n",
    "        'index_edit_ops': ix_edit_ops\n",
    "    }).astype({'ref_ix': 'Int32', 'hyp_ix': 'Int32', 'index_edit_ops': 'Int32'})\n",
    "\n",
    "\n",
    "    return aligned_df\n",
    "# apply align_words to whisper_norm_asr and phi4_norm_asr\n",
    "evaluate_data['whisper_aligned_df'] = evaluate_data.apply(\n",
    "    lambda row: align_words(row['norm_human_transcript'], row['norm_whisper_asr'], None), axis=1\n",
    ")\n",
    "evaluate_data['phi4_aligned_df'] = evaluate_data.apply(\n",
    "    lambda row: align_words(row['norm_human_transcript'], row['norm_phi4_asr'], None), axis=1\n",
    ")\n",
    "evaluate_data['parakeet_aligned_df'] = evaluate_data.apply(\n",
    "    lambda row: align_words(row['norm_human_transcript'], row['norm_parakeet_asr'], None), axis=1\n",
    ")\n",
    "# evaluate_data['granite_aligned_df'] = evaluate_data.apply(\n",
    "#     lambda row: align_words(row['norm_human_transcript'], row['norm_granite'], None), axis=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "368090cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use output of aligned words to analyze equal, deletions, insertions, substitutions and reconstruct the reference text while marking the errors\n",
    "\n",
    "def reconstruct_reference_with_errors(aligned_df):\n",
    "    # handle None, non-DataFrame, and empty\n",
    "    if aligned_df is None or not isinstance(aligned_df, pd.DataFrame) or aligned_df.empty:\n",
    "        return \"\"\n",
    "    \n",
    "    reconstructed = []\n",
    "    for _, row in aligned_df.iterrows():\n",
    "        op = row.get('operation')\n",
    "        ref_word = row.get('reference', '')\n",
    "        hyp_word = row.get('hypothesis', '')\n",
    "        if op == '=':\n",
    "            reconstructed.append(ref_word)\n",
    "        elif op == 'ins':\n",
    "            # show the inserted hypothesis word instead of '_' placeholder\n",
    "            reconstructed.append(f\"[INS:{hyp_word}]\")\n",
    "        elif op == 'del':\n",
    "            reconstructed.append(f\"[DEL:{ref_word}]\")\n",
    "        elif op == 'sub':\n",
    "            reconstructed.append(f\"[SUB:{ref_word}->{hyp_word}]\")\n",
    "    return ' '.join(w for w in reconstructed if isinstance(w, str))\n",
    "\n",
    "# apply to all aligned dfs\n",
    "evaluate_data['whisper_reconstructed_ref'] = evaluate_data['whisper_aligned_df'].apply(reconstruct_reference_with_errors)\n",
    "evaluate_data['phi4_reconstructed_ref'] = evaluate_data['phi4_aligned_df'].apply(reconstruct_reference_with_errors)\n",
    "evaluate_data['parakeet_reconstructed_ref'] = evaluate_data['parakeet_aligned_df'].apply(reconstruct_reference_with_errors)\n",
    "# evaluate_data['granite_reconstructed_ref'] = evaluate_data['granite_aligned_df'].apply(reconstruct_reference_with_errors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35703fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Reconstruct human transcript based on ASR outputs and alignment ops; color-code the errors encountered\n",
    "# def reconstruct_human_transcript(hyp_text, alignment_ops):\n",
    "#     reconstructed = []\n",
    "#     hyp_words = hyp_text.split() if isinstance(hyp_text, str) else []\n",
    "\n",
    "#     # if hypothesis empty -> return empty\n",
    "#     if not hyp_words:\n",
    "#         return \"\"\n",
    "\n",
    "#     processed = preprocess_alignment_ops(alignment_ops)\n",
    "\n",
    "#     # produce a flat list of op entries regardless of input shape\n",
    "#     if isinstance(processed, list):\n",
    "#         # flatten one level (handles [[...]] case)\n",
    "#         flat = []\n",
    "#         for item in processed:\n",
    "#             if isinstance(item, list):\n",
    "#                 flat.extend(item)\n",
    "#             else:\n",
    "#                 flat.append(item)\n",
    "#         alignment_ops_list = flat\n",
    "#     elif isinstance(processed, dict):\n",
    "#         alignment_ops_list = [processed]\n",
    "#     else:\n",
    "#         if not processed:\n",
    "#             return \"\"\n",
    "#         try:\n",
    "#             alignment_ops_list = ast.literal_eval(processed)\n",
    "#             # if parsed to nested list, flatten one level\n",
    "#             if isinstance(alignment_ops_list, list) and any(isinstance(x, list) for x in alignment_ops_list):\n",
    "#                 flat = []\n",
    "#                 for item in alignment_ops_list:\n",
    "#                     if isinstance(item, list):\n",
    "#                         flat.extend(item)\n",
    "#                     else:\n",
    "#                         flat.append(item)\n",
    "#                 alignment_ops_list = flat\n",
    "#         except Exception as e:\n",
    "#             raise ValueError(f\"Failed to parse alignment_ops: {e}\")\n",
    "\n",
    "#     for op in alignment_ops_list:\n",
    "#         typ = _get_field(op, \"type\")\n",
    "#         if typ == \"delete\":\n",
    "#             s = _get_field(op, \"ref_start_idx\", 0)\n",
    "#             e = _get_field(op, \"ref_end_idx\", 0)\n",
    "#             deleted_words = f\"[DEL: {' '.join(['__'+w+'__' for w in hyp_words[s:e]])}]\"\n",
    "#             reconstructed.append(deleted_words)\n",
    "#         elif typ == \"equal\":\n",
    "#             s = _get_field(op, \"hyp_start_idx\", 0)\n",
    "#             e = _get_field(op, \"hyp_end_idx\", 0)\n",
    "#             reconstructed.extend(hyp_words[s:e])\n",
    "#         elif typ == \"insert\":\n",
    "#             s = _get_field(op, \"hyp_start_idx\", 0)\n",
    "#             e = _get_field(op, \"hyp_end_idx\", 0)\n",
    "#             inserted_words = f\"[INS: {' '.join(['++'+w+'++' for w in hyp_words[s:e]])}]\"\n",
    "#             reconstructed.append(inserted_words)\n",
    "#         elif typ == \"substitute\":\n",
    "#             s = _get_field(op, \"ref_start_idx\", 0)\n",
    "            \n",
    "#             e = _get_field(op, \"ref_end_idx\", 0)\n",
    "#             substituted_words = f\"[SUB: {' '.join(['~~'+w+'~~' for w in hyp_words[s:e]])}]\"\n",
    "#             reconstructed.append(substituted_words)\n",
    "#     return ' '.join(reconstructed)\n",
    "\n",
    "# # apply reconstruct_human_transcript to each ASR column\n",
    "# for asr_col, ops_col in zip(asr_rows, align_ops_rows):\n",
    "#     evaluate_data[f'{asr_col}_reconstructed_human'] = evaluate_data.apply(\n",
    "#         lambda row: reconstruct_human_transcript(row['norm_human_transcript'], row[ops_col]), axis=1\n",
    "#     )\n",
    "\n",
    "# # # apply to whipser ASR only for now\n",
    "# # evaluate_data['norm_whisper_asr_reconstructed_human'] = evaluate_data.apply(\n",
    "# #     lambda row: reconstruct_human_transcript(row['norm_human_transcript'], row['norm_whisper_asr_ops']), axis=1\n",
    "# # )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4756f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reconstruct human transcript for primock as well\n",
    "# primock_asr_rows = ['norm_whisper_doctor', 'norm_whisper_patient', 'norm_phi4_doctor', 'norm_phi4_patient',\n",
    "#                     'norm_parakeet_doctor', 'norm_parakeet_patient', 'norm_granite_doctor', 'norm_granite_patient']\n",
    "# primock_align_ops_rows = ['norm_whisper_doctor_ops', 'norm_whisper_patient_ops', 'norm_phi4_doctor_ops', 'norm_phi4_patient_ops', 'norm_parakeet_doctor_ops', 'norm_parakeet_patient_ops', 'norm_granite_doctor_ops', 'norm_granite_patient_ops']\n",
    "# for asr_col, ops_col in zip(primock_asr_rows, primock_align_ops_rows):\n",
    "#     primock_merged[f'{asr_col}_reconstructed_human'] = primock_merged.apply(\n",
    "#         lambda row: reconstruct_human_transcript(\n",
    "#             row['norm_human_doctor'] if 'doctor' in asr_col else row['norm_human_patient'],\n",
    "#             row[ops_col]\n",
    "#         ), axis=1\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "870c799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply align_words to primock as well\n",
    "# primock_merged['whisper_doctor_aligned_df'] = primock_merged.apply(\n",
    "#     lambda row: align_words(row['norm_human_doctor'], row['norm_whisper_doctor'], None), axis=1\n",
    "# )\n",
    "# primock_merged['whisper_patient_aligned_df'] = primock_merged.apply(\n",
    "#     lambda row: align_words(row['norm_human_patient'], row['norm_whisper_patient'], None), axis=1\n",
    "# )\n",
    "# primock_merged['phi4_doctor_aligned_df'] = primock_merged.apply(\n",
    "#     lambda row: align_words(row['norm_human_doctor'], row['norm_phi4_doctor'], None), axis=1\n",
    "# )\n",
    "# primock_merged['phi4_patient_aligned_df'] = primock_merged.apply(\n",
    "#     lambda row: align_words(row['norm_human_patient'], row['norm_phi4_patient'], None), axis=1\n",
    "# )\n",
    "# primock_merged['parakeet_doctor_aligned_df'] = primock_merged.apply(\n",
    "#     lambda row: align_words(row['norm_human_doctor'], row['norm_parakeet_doctor'], None), axis=1\n",
    "# )\n",
    "# primock_merged['parakeet_patient_aligned_df'] = primock_merged.apply(\n",
    "#     lambda row: align_words(row['norm_human_patient'], row['norm_parakeet_patient'], None), axis=1\n",
    "# )\n",
    "# primock_merged['granite_doctor_aligned_df'] = primock_merged.apply(\n",
    "#     lambda row: align_words(row['norm_human_doctor'], row['norm_granite_doctor'], None), axis=1\n",
    "# )\n",
    "# primock_merged['granite_patient_aligned_df'] = primock_merged.apply(\n",
    "#     lambda row: align_words(row['norm_human_patient'], row['norm_granite_patient'], None), axis=1\n",
    "# )\n",
    "\n",
    "# # apply reconstruct_reference_with_errors to all aligned dfs in primock\n",
    "# primock_merged['whisper_doctor_reconstructed_ref'] = primock_merged['whisper_doctor_aligned_df'].apply(reconstruct_reference_with_errors)\n",
    "# primock_merged['whisper_patient_reconstructed_ref'] = primock_merged['whisper_patient_aligned_df'].apply(reconstruct_reference_with_errors)\n",
    "# primock_merged['phi4_doctor_reconstructed_ref'] = primock_merged['phi4_doctor_aligned_df'].apply(reconstruct_reference_with_errors)\n",
    "# primock_merged['phi4_patient_reconstructed_ref'] = primock_merged['phi4_patient_aligned_df'].apply(reconstruct_reference_with_errors)\n",
    "# primock_merged['parakeet_doctor_reconstructed_ref'] = primock_merged['parakeet_doctor_aligned_df'].apply(reconstruct_reference_with_errors)\n",
    "# primock_merged['parakeet_patient_reconstructed_ref'] = primock_merged['parakeet_patient_aligned_df'].apply(reconstruct_reference_with_errors)\n",
    "# primock_merged['granite_doctor_reconstructed_ref'] = primock_merged['granite_doctor_aligned_df'].apply(reconstruct_reference_with_errors)\n",
    "# primock_merged['granite_patient_reconstructed_ref'] = primock_merged['granite_patient_aligned_df'].apply(reconstruct_reference_with_errors) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9deebf7",
   "metadata": {},
   "source": [
    "## Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b08e8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-essential columns that ends with _asr_wer_compute, and _asr_ops\n",
    "cols_to_drop = [col for col in evaluate_data.columns if col.endswith('_asr_wer_compute') or col.endswith('_asr_ops')]\n",
    "evaluate_data.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# save the dataframe to a new excel file\n",
    "evaluate_data.to_excel('results/all_result_processed_normalized.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "957b4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the excel file and display the first few rows\n",
    "df = pd.read_excel('results/all_result_processed_normalized.xlsx', engine='openpyxl')\n",
    "\n",
    "# move each models' results to separate sheets in the excel file\n",
    "with pd.ExcelWriter('results/all_result_separate_sheets_normalized.xlsx', engine='openpyxl') as writer:\n",
    "    whisper_cols = ['utterance_id', 'source', 'duration_sec', 'human-transcript', 'Whisper-ASR', 'norm_human_transcript', 'norm_whisper_asr', \n",
    "                    'norm_whisper_asr_wer', 'norm_whisper_asr_ins', 'norm_whisper_asr_del', \n",
    "                    'norm_whisper_asr_sub', 'whisper_aligned_df',\n",
    "                    'norm_whisper_asr_Deletions', 'norm_whisper_asr_Insertions', 'norm_whisper_asr_Substitutions', 'whisper_reconstructed_ref']\n",
    "    phi4_cols = ['utterance_id', 'source', 'duration_sec', 'human-transcript', 'Phi-4-ASR', 'norm_human_transcript', 'norm_phi4_asr', \n",
    "                 'norm_phi4_asr_wer', 'norm_phi4_asr_ins', 'norm_phi4_asr_del', \n",
    "                 'norm_phi4_asr_sub', 'phi4_aligned_df',\n",
    "                 'norm_phi4_asr_Deletions', 'norm_phi4_asr_Insertions', 'norm_phi4_asr_Substitutions', 'phi4_reconstructed_ref']\n",
    "    parakeet_cols = ['utterance_id', 'source', 'duration_sec', 'human-transcript', 'Nvidia-Parakeet-ASR', 'norm_human_transcript', 'norm_parakeet_asr', \n",
    "                     'norm_parakeet_wer', 'norm_parakeet_ins', 'norm_parakeet_del', \n",
    "                     'norm_parakeet_sub', 'parakeet_aligned_df',\n",
    "                     'norm_parakeet_Deletions', 'norm_parakeet_Insertions', 'norm_parakeet_Substitutions', 'parakeet_reconstructed_ref']\n",
    "    # granite_cols = ['utterance_id', 'source', 'duration_sec', 'human-transcript', 'IBM-Granite', 'norm_human_transcript', 'norm_granite', \n",
    "    #                 'norm_granite_wer', 'norm_granite_ins', 'norm_granite_del', \n",
    "    #                 'norm_granite_sub', 'granite_aligned_df',\n",
    "    #                 'norm_granite_Deletions', 'norm_granite_Insertions', 'norm_granite_Substitutions', 'granite_reconstructed_ref']\n",
    "    df_whisper = df.filter(items=whisper_cols, axis=1)\n",
    "    df_phi4 = df.filter(items=phi4_cols, axis=1)\n",
    "    df_parakeet = df.filter(items=parakeet_cols, axis=1)\n",
    "    # df_granite = df.filter(items=granite_cols, axis=1)\n",
    "\n",
    "    # Only write non-empty DataFrames to avoid invisible sheet error\n",
    "    if not df_whisper.empty:\n",
    "        df_whisper.to_excel(writer, sheet_name='Whisper-ASR Results', index=False)\n",
    "    if not df_phi4.empty:\n",
    "        df_phi4.to_excel(writer, sheet_name='Phi-4-ASR Results', index=False)\n",
    "    if not df_parakeet.empty:\n",
    "        df_parakeet.to_excel(writer, sheet_name='Nvidia-Parakeet-ASR Results', index=False)\n",
    "    # if not df_granite.empty:\n",
    "    #     df_granite.to_excel(writer, sheet_name='IBM-Granite Results', index=False)\n",
    "        \n",
    "    # resize each row in the sheets to 120px\n",
    "    for sheet_name in ['Whisper-ASR Results', 'Phi-4-ASR Results', 'Nvidia-Parakeet-ASR Results']: #'Nvidia-Parakeet-ASR Results', 'IBM-Granite Results'\n",
    "        worksheet = writer.sheets.get(sheet_name)\n",
    "        if worksheet:\n",
    "            for row_idx in range(1, len(df) + 2):  # +2 to account for header row and 1-based indexing\n",
    "                worksheet.row_dimensions[row_idx].height = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98b28836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # seperate primock results into separate sheets as well\n",
    "# with pd.ExcelWriter('primock_result_separate_sheets.xlsx', engine='openpyxl') as writer:\n",
    "#     whisper_cols = ['conversation_id', 'turns', 'doctor_utterances', 'patient_utterances', 'norm_human_doctor', 'norm_human_patient', 'norm_whisper_doctor', \n",
    "#                     'norm_whisper_patient', 'norm_whisper_doctor_wer', 'norm_whisper_doctor_ins', 'norm_whisper_doctor_del', \n",
    "#                     'norm_whisper_doctor_sub', 'whisper_doctor_aligned_df',\n",
    "#                     'norm_whisper_doctor_Deletions', 'norm_whisper_doctor_Insertions', 'norm_whisper_doctor_Substitutions', 'whisper_doctor_reconstructed_ref',\n",
    "#                     'norm_whisper_patient_wer', 'norm_whisper_patient_ins', 'norm_whisper_patient_del', \n",
    "#                     'norm_whisper_patient_sub', 'whisper_patient_aligned_df',\n",
    "#                     'norm_whisper_patient_Deletions', 'norm_whisper_patient_Insertions', 'norm_whisper_patient_Substitutions', 'whisper_patient_reconstructed_ref']\n",
    "#     phi4_cols = ['conversation_id', 'turns', 'doctor_utterances', 'patient_utterances', 'norm_human_doctor', 'norm_human_patient', 'norm_phi4_doctor', \n",
    "#                  'norm_phi4_patient', 'norm_phi4_doctor_wer', 'norm_phi4_doctor_ins', 'norm_phi4_doctor_del', \n",
    "#                  'norm_phi4_doctor_sub', 'phi4_doctor_aligned_df',\n",
    "#                  'norm_phi4_doctor_Deletions', 'norm_phi4_doctor_Insertions', 'norm_phi4_doctor_Substitutions', 'phi4_doctor_reconstructed_ref',\n",
    "#                  'norm_phi4_patient_wer', 'norm_phi4_patient_ins', 'norm_phi4_patient_del', \n",
    "#                  'norm_phi4_patient_sub', 'phi4_patient_aligned_df',\n",
    "#                  'norm_phi4_patient_Deletions', 'norm_phi4_patient_Insertions', 'norm_phi4_patient_Substitutions', 'phi4_patient_reconstructed_ref']\n",
    "#     parakeet_cols = ['conversation_id', 'turns', 'doctor_utterances', 'patient_utterances', 'norm_human_doctor', 'norm_human_patient', 'norm_parakeet_doctor', \n",
    "#                      'norm_parakeet_patient', 'norm_parakeet_doctor_wer', 'norm_parakeet_doctor_ins', 'norm_parakeet_doctor_del', \n",
    "#                      'norm_parakeet_doctor_sub', 'parakeet_doctor_aligned_df',\n",
    "#                      'norm_parakeet_doctor_Deletions', 'norm_parakeet_doctor_Insertions', 'norm_parakeet_doctor_Substitutions', 'parakeet_doctor_reconstructed_ref',\n",
    "#                      'norm_parakeet_patient_wer', 'norm_parakeet_patient_ins', 'norm_parakeet_patient_del', \n",
    "#                      'norm_parakeet_patient_sub', 'parakeet_patient_aligned_df',\n",
    "#                      'norm_parakeet_patient_Deletions', 'norm_parakeet_patient_Insertions', 'norm_parakeet_patient_Substitutions', 'parakeet_patient_reconstructed_ref']\n",
    "#     granite_cols = ['conversation_id', 'turns', 'doctor_utterances', 'patient_utterances', 'norm_human_doctor', 'norm_human_patient', 'norm_granite_doctor', \n",
    "#                     'norm_granite_patient', 'norm_granite_doctor_wer', 'norm_granite_doctor_ins', 'norm_granite_doctor_del', \n",
    "#                     'norm_granite_doctor_sub', 'granite_doctor_aligned_df',\n",
    "#                     'norm_granite_doctor_Deletions', 'norm_granite_doctor_Insertions', 'norm_granite_doctor_Substitutions', 'granite_doctor_reconstructed_ref',\n",
    "#                     'norm_granite_patient_wer', 'norm_granite_patient_ins', 'norm_granite_patient_del', \n",
    "#                     'norm_granite_patient_sub', 'granite_patient_aligned_df',\n",
    "#                     'norm_granite_patient_Deletions', 'norm_granite_patient_Insertions', 'norm_granite_patient_Substitutions', 'granite_patient_reconstructed_ref']\n",
    "#     df_whisper = primock_merged.filter(items=whisper_cols, axis=1)\n",
    "#     df_phi4 = primock_merged.filter(items=phi4_cols, axis=1)\n",
    "#     df_parakeet = primock_merged.filter(items=parakeet_cols, axis=1)\n",
    "#     df_granite = primock_merged.filter(items=granite_cols, axis=1)\n",
    "#     if not df_whisper.empty:\n",
    "#         df_whisper.to_excel(writer, sheet_name='Whisper-ASR Results', index=False)\n",
    "#     if not df_phi4.empty:\n",
    "#         df_phi4.to_excel(writer, sheet_name='Phi-4-ASR Results', index=False)\n",
    "#     if not df_parakeet.empty:\n",
    "#         df_parakeet.to_excel(writer, sheet_name='Nvidia-Parakeet-ASR Results', index=False)\n",
    "#     if not df_granite.empty:\n",
    "#         df_granite.to_excel(writer, sheet_name='IBM-Granite Results', index=False)\n",
    "    \n",
    "# # resize each row in the sheets to 120px\n",
    "#     for sheet_name in ['Whisper-ASR Results', 'Phi-4-ASR Results', 'Nvidia-Parakeet-ASR Results', 'IBM-Granite Results']:\n",
    "#         worksheet = writer.sheets.get(sheet_name)\n",
    "#         if worksheet:\n",
    "#             for row_idx in range(1, len(primock_merged) + 2):  # +2 to account for header row and 1-based indexing\n",
    "#                 worksheet.row_dimensions[row_idx].height = 120\n",
    "    \n",
    "    \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "271d29c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_572293/304160582.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_data.rename(columns={'phi-4-asr': 'phi4-asr'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# select three session with the following utterance ids: day4_consultation07, 1_Malaria, RES0073\n",
    "selected_utterances = ['2_Diarrhea', '18_Pneumonia', '46aacf84-fdd1-490b-a857-633d2e7763a0_7d4de4c9d3488a4bbd35634cbd3a2b66_l1RjPEwA']\n",
    "selected_data = evaluate_data[evaluate_data['utterance_id'].isin(selected_utterances)]\n",
    "\n",
    "# make all columns lowercase for better readability\n",
    "selected_data.columns = [col.lower() for col in selected_data.columns]\n",
    "\n",
    "# rename \"phi-4-asr\" column to \"phi4-asr\" for better readability\n",
    "selected_data.rename(columns={'phi-4-asr': 'phi4-asr'}, inplace=True)\n",
    "\n",
    "# save the selected data to a new excel file\n",
    "selected_data.to_excel('results/selected_sessions_normalized.xlsx', index=False, engine='openpyxl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi4_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
